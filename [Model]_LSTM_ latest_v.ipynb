{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_v7.5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s_EovrYJdEx",
        "outputId": "b023a742-3d0f-412d-d013-af95a6e110e0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit\n",
        "from tensorflow.keras.models import Sequential, load_model, save_model, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error\n",
        "from math import sqrt\n",
        "import os\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "\n",
        "# GPU 사용 여부 확인\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.test.is_gpu_available(\n",
        "    cuda_only=False,\n",
        "    min_cuda_compute_capability=None\n",
        ")\n",
        "\n",
        "\n",
        "# 구글드라이브 여부 확인 및 구글 드라이브 설정\n",
        "import sys\n",
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "if IS_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# COLAB에서 사용하는 경우 default 경로 지정\n",
        "if IS_COLAB:\n",
        "    DEFAULT_PATH = \"/content/drive/My Drive/\"\n",
        "else:\n",
        "    DEFAULT_PATH = ''\n",
        "\n",
        "#############################################################################\n",
        "# pickle 오류 대응\n",
        "# https://github.com/tensorflow/tensorflow/issues/34697\n",
        "from tensorflow.python.keras.saving import saving_utils\n",
        "from tensorflow.python.keras.layers import deserialize, serialize\n",
        "\n",
        "def unpack(model, training_config, weights):\n",
        "    restored_model = deserialize(model)\n",
        "    if training_config is not None:\n",
        "        restored_model.compile(\n",
        "            **saving_utils.compile_args_from_training_config(\n",
        "                training_config\n",
        "            )\n",
        "        )\n",
        "    restored_model.set_weights(weights)\n",
        "    return restored_model\n",
        "\n",
        "## Hotfix function\n",
        "def make_keras_picklable():\n",
        "\n",
        "    def __reduce__(self):\n",
        "        model_metadata = saving_utils.model_metadata(self)\n",
        "        training_config = model_metadata.get(\"training_config\", None)\n",
        "        model = serialize(self)\n",
        "        weights = self.get_weights()\n",
        "        return (unpack, (model, training_config, weights))\n",
        "\n",
        "    cls = Model\n",
        "    cls.__reduce__ = __reduce__\n",
        "\n",
        "## Run the function\n",
        "make_keras_picklable()\n",
        "#############################################################################"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 4666173976165609858\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11344216064\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 14905598556076369340\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n",
            "2.6.0\n",
            "WARNING:tensorflow:From <ipython-input-2-31d23f876766>:26: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e7OfJAeLDZT"
      },
      "source": [
        "def pr_weather(ds, week=False):\n",
        "    \"\"\"\n",
        "    기상 데이터 전처리\n",
        "\n",
        "    생육 적정 온도에서 벗어나는 값, \n",
        "    발아 한계 기온에서 벗어나는 값, \n",
        "    태풍 여부 등을 처리함\n",
        "    \"\"\"\n",
        "    weather = ds.copy()\n",
        "    weather[\"관측시각\"] = pd.to_datetime(weather[\"관측시각\"], format=\"%Y-%m-%d\")\n",
        "    weather.drop(columns=[\"연도\", \"월\", \"일\"], inplace=True)\n",
        "    weather.sort_values(\"관측시각\", inplace=True)\n",
        "    \n",
        "    # 생육 적정온도 15 ~ 25도\n",
        "    # 최적기온을 벗어나는 값을 최저기온, 평균기온, 최고기온에 따라\n",
        "    # 가중치 값을 달리함\n",
        "    weather[\"!최적기온\"] = 0 \n",
        "    temp_index = \\\n",
        "        weather[\n",
        "            (weather[\"최저기온\"] < 15)\n",
        "            | (weather[\"최고기온\"] > 25)\n",
        "        ].index\n",
        "    weather.loc[temp_index, \"!최적기온\"] += 1\n",
        "\n",
        "    temp_index = \\\n",
        "        weather[\n",
        "            (weather[\"기온\"] < 15)\n",
        "            | (weather[\"기온\"] > 25)\n",
        "        ].index\n",
        "    weather.loc[temp_index, \"!최적기온\"] += 5\n",
        "\n",
        "    temp_index = \\\n",
        "        weather[\n",
        "            (weather[\"최고기온\"] < 15)\n",
        "            | (weather[\"최저기온\"] > 25)\n",
        "        ].index\n",
        "    weather.loc[temp_index, \"!최적기온\"] += 10\n",
        "\n",
        "    # 발아 한계 기온\n",
        "    # 발아 한계 기온에서 벗어나는 값을 최저기온, 평균기온, 최고기온에 따라\n",
        "    # 가중치 값을 달리함\n",
        "    weather[\"불량기온\"] = 0 \n",
        "    temp_index = \\\n",
        "        weather[\n",
        "            (weather[\"최저기온\"] < 0)\n",
        "            | (weather[\"최고기온\"] > 33)\n",
        "        ].index\n",
        "    weather.loc[temp_index, \"불량기온\"] += 1\n",
        "\n",
        "    temp_index = \\\n",
        "        weather[\n",
        "            (weather[\"기온\"] < 0)\n",
        "            | (weather[\"기온\"] > 33)\n",
        "        ].index\n",
        "    weather.loc[temp_index, \"불량기온\"] += 5\n",
        "\n",
        "    temp_index = \\\n",
        "        weather[\n",
        "            (weather[\"최고기온\"] < 0)\n",
        "            | (weather[\"최저기온\"] > 33)\n",
        "        ].index\n",
        "    weather.loc[temp_index, \"불량기온\"] += 10\n",
        "\n",
        "    # 강수량이 90을 초과하고 풍속이 4 이상이면 태풍이라고 판단함\n",
        "    weather[\"태풍\"] = 0\n",
        "    weather.loc[\n",
        "        (weather[\"강수량\"] > 90) & (weather[\"풍속\"] > 4),\n",
        "        \"태풍\"\n",
        "    ] = 1    \n",
        "\n",
        "    agg_dict = {\n",
        "        \"기온\": [(\"기온\", \"mean\")],\n",
        "#         \"최고기온\": [(\"최고기온\", \"max\")],\n",
        "#         \"최저기온\": [(\"최저기온\", \"min\")],\n",
        "#         \"습도\": [(\"습도\", \"mean\")],\n",
        "#         \"풍속\": [(\"풍속\", \"mean\")],\n",
        "        \"일조시간\": [(\"일조시간\", \"sum\")],\n",
        "        \"일사량\": [(\"일사량\", \"sum\")],\n",
        "        \"결로시간\": [(\"결로시간\", \"sum\")],\n",
        "        \"!최적기온\": [(\"!최적기온\", \"sum\")],\n",
        "        \"불량기온\": [(\"불량기온\", \"sum\")],\n",
        "        \"태풍\": [(\"태풍\", \"sum\")],\n",
        "    }\n",
        "    \n",
        "    if week:\n",
        "        # 연도와 주번호 생성\n",
        "        weather[\"WEEK_N\"] = weather[\"관측시각\"].dt.strftime('%U')\n",
        "        weather[\"YEAR\"] = weather[\"관측시각\"].dt.year\n",
        "        weather_g = \\\n",
        "            weather.groupby([\"관측지점명\", \"YEAR\", \"WEEK_N\"]).agg(agg_dict)\n",
        "    else:\n",
        "        weather_g = \\\n",
        "            weather.groupby([\"관측지점명\", \"관측시각\"]).agg(agg_dict)\n",
        "    weather_g.reset_index(col_level=1, inplace=True)\n",
        "    weather_g = weather_g.droplevel(level=0, axis=1)\n",
        "    \n",
        "    # 지역명 변경\n",
        "    loca_dict = {\n",
        "        \"진도군 군내면\": \"전라남도(진도+)\",\n",
        "        \"부산시 강서구\": \"경상남도(부산+)\",\n",
        "        \"평창군 여만리\": \"강원도(평창+)\",\n",
        "        \"남양주 진건읍\": \"경기도(구리+)\"\n",
        "    }\n",
        "\n",
        "    for loc in loca_dict.keys():\n",
        "        weather_g.loc[weather_g[\"관측지점명\"] == loc, \"관측지점명\"] = loca_dict[loc]\n",
        "\n",
        "    return weather_g"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTYTVBuDLDZU"
      },
      "source": [
        "def pr_main(ds, drop_leap_year=True):\n",
        "    \"\"\"\n",
        "    기본적인 데이터 처리\n",
        "\n",
        "    drop_leap_year=True: 윤년(leap year) 2월 29일 데이터 삭제\n",
        "    \"\"\"\n",
        "    main = ds.copy()\n",
        "    \n",
        "    main[\"DELNG_DE\"] = pd.to_datetime(main[\"DELNG_DE\"], format=\"%Y%m%d\")\n",
        "    drop_columns = \\\n",
        "        [\n",
        "         \"PBLMNG_WHSAL_MRKT_NM\", \"CPR_NM\", \n",
        "         \"DELNGBUNDLE_QY\", \"STNDRD\", \"DELNG_QY\"\n",
        "        ]\n",
        "    main.drop(columns=drop_columns, inplace=True)\n",
        "    main.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # 윤년(leap year) 2월 29일 삭제\n",
        "    if drop_leap_year:\n",
        "        drop_index = \\\n",
        "            main.loc[\n",
        "                (main[\"DELNG_DE\"].dt.month == 2)\n",
        "                & (main[\"DELNG_DE\"].dt.day == 29)\n",
        "            ].index\n",
        "        main.drop(drop_index, inplace=True)\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    return main\n",
        "\n",
        "# 메인 데이터 일별 그룹화\n",
        "def main_to_day(ds):\n",
        "    main = ds.copy()\n",
        "    def q80(x):\n",
        "        return x.quantile(0.8)\n",
        "    def q20(x):\n",
        "        return x.quantile(0.2)\n",
        "\n",
        "    # 월 더미 추가\n",
        "    month_dumm = pd.get_dummies(main[\"DELNG_DE\"].dt.month_name())\n",
        "    main = pd.concat([main, month_dumm], axis=1)\n",
        "\n",
        "    agg_dict = {\n",
        "        \"PRICE\": [\n",
        "            # (\"P_MIN\", \"min\"),\n",
        "            # (\"P_MAX\", \"max\"),\n",
        "            (\"P_MEDIAN\", np.median),\n",
        "            (\"P_MEAN\", np.mean),\n",
        "            (\"P_STD\", np.std),\n",
        "            (\"P_Q80\", q80),\n",
        "            (\"P_Q20\", q20),\n",
        "        ],\n",
        "        \"VOLUME\": [\n",
        "            (\"VOLUME\", \"sum\"),\n",
        "        ],\n",
        "        \"January\": [(\"January\", \"mean\")],\n",
        "        \"February\": [(\"February\", \"mean\")],\n",
        "        \"March\": [(\"March\", \"mean\")],\n",
        "        \"April\": [(\"April\", \"mean\")],\n",
        "        \"May\": [(\"May\", \"mean\")],\n",
        "        \"June\": [(\"June\", \"mean\")],\n",
        "        \"July\": [(\"July\", \"mean\")],\n",
        "        \"August\": [(\"August\", \"mean\")],\n",
        "        \"September\": [(\"September\", \"mean\")],\n",
        "        \"October\": [(\"October\", \"mean\")],\n",
        "        \"November\": [(\"November\", \"mean\")],\n",
        "        \"December\": [(\"December\", \"mean\")]\n",
        "    }\n",
        "\n",
        "    main_g = main.groupby([\"SANJI_NM\", \"DELNG_DE\"]).agg(agg_dict).copy()\n",
        "    main_g.reset_index(col_level=1, inplace=True)\n",
        "    main_g = main_g.droplevel(level=0, axis=1)\n",
        "\n",
        "    # 표준편차 결측값 0으로 대치\n",
        "    main_g[\"P_STD\"].fillna(0, inplace=True)\n",
        "    \n",
        "    # # weekday 추가\n",
        "    # weekday_dumm = pd.get_dummies(main_g[\"DELNG_DE\"].dt.weekday)\n",
        "    # weekday_dumm.columns = [\"MON\", \"TUE\", \"WED\", \"THU\", \"FRI\", \"SAT\", \"SUN\"]\n",
        "    # main_g = pd.concat([main_g, weekday_dumm], axis=1)\n",
        "\n",
        "    # # 주번호(일요일 기준) 추가\n",
        "    # main_g[\"WEEK_N\"] = main_g[\"DELNG_DE\"].dt.strftime('%U')\n",
        "\n",
        "    # # 연 추가\n",
        "    # main_g[\"YEAR\"] = main_g[\"DELNG_DE\"].dt.year\n",
        "\n",
        "    # 이동평균\n",
        "    main_g[\"P_MEDIAN\"].rolling(5, min_periods=1).mean()\n",
        "\n",
        "    # 중앙가격 지수이동평균 (Exponetial Moving Average)\n",
        "    main_g[\"P_EMA5\"] = main_g[\"P_MEDIAN\"].ewm(5, min_periods=1).mean()\n",
        "    main_g[\"P_EMA20\"] = main_g[\"P_MEDIAN\"].ewm(20, min_periods=1).mean()\n",
        "    main_g[\"P_EMA45\"] = main_g[\"P_MEDIAN\"].ewm(45, min_periods=1).mean()\n",
        "    main_g[\"P_EMA60\"] = main_g[\"P_MEDIAN\"].ewm(60, min_periods=1).mean()\n",
        "    main_g[\"P_EMA90\"] = main_g[\"P_MEDIAN\"].ewm(90, min_periods=1).mean()\n",
        "\n",
        "    # 거래량 지수이동평균 (Exponetial Moving Average)\n",
        "    main_g[\"V_EMA5\"] = main_g[\"VOLUME\"].ewm(5, min_periods=1).mean()\n",
        "    main_g[\"V_EMA20\"] = main_g[\"VOLUME\"].ewm(20, min_periods=1).mean()\n",
        "    main_g[\"V_EMA45\"] = main_g[\"VOLUME\"].ewm(45, min_periods=1).mean()\n",
        "    main_g[\"V_EMA60\"] = main_g[\"VOLUME\"].ewm(60, min_periods=1).mean()\n",
        "    main_g[\"V_EMA90\"] = main_g[\"VOLUME\"].ewm(90, min_periods=1).mean()\n",
        "    \n",
        "    return main_g\n",
        "\n",
        "# 메인 데이터 주별 그룹화\n",
        "def main_to_week(ds):\n",
        "    main = ds.copy()\n",
        "    \n",
        "    main.sort_values(\"DELNG_DE\", inplace=True)\n",
        "    # 주번호(일요일 기준)\n",
        "    main[\"WEEK_N\"] = main[\"DELNG_DE\"].dt.strftime('%U')\n",
        "    # 연도\n",
        "    main[\"YEAR\"] = main[\"DELNG_DE\"].dt.year\n",
        "    \n",
        "    # 월 더미 추가\n",
        "    month_dumm = pd.get_dummies(main[\"DELNG_DE\"].dt.month_name())\n",
        "    main = pd.concat([main, month_dumm], axis=1)\n",
        "\n",
        "    # 그룹화\n",
        "    def q80(x):\n",
        "        return x.quantile(0.8)\n",
        "    def q20(x):\n",
        "        return x.quantile(0.2)\n",
        "\n",
        "    agg_dict = {\n",
        "        \"PRICE\": [\n",
        "            # (\"P_MIN\", \"min\"),\n",
        "            # (\"P_MAX\", \"max\"),\n",
        "            (\"P_MEDIAN\", np.median),\n",
        "            (\"P_MEAN\", np.mean),\n",
        "            (\"P_STD\", np.std),\n",
        "            (\"P_Q80\", q80),\n",
        "            (\"P_Q20\", q20),\n",
        "        ],\n",
        "        \"VOLUME\": [\n",
        "            (\"VOLUME\", \"sum\"),\n",
        "            (\"V_STD\", np.std)\n",
        "        ],\n",
        "        \"January\": [(\"January\", \"mean\")],\n",
        "        \"February\": [(\"February\", \"mean\")],\n",
        "        \"March\": [(\"March\", \"mean\")],\n",
        "        \"April\": [(\"April\", \"mean\")],\n",
        "        \"May\": [(\"May\", \"mean\")],\n",
        "        \"June\": [(\"June\", \"mean\")],\n",
        "        \"July\": [(\"July\", \"mean\")],\n",
        "        \"August\": [(\"August\", \"mean\")],\n",
        "        \"September\": [(\"September\", \"mean\")],\n",
        "        \"October\": [(\"October\", \"mean\")],\n",
        "        \"November\": [(\"November\", \"mean\")],\n",
        "        \"December\": [(\"December\", \"mean\")]\n",
        "    }\n",
        "    main_g = main.groupby([\"SANJI_NM\", \"YEAR\", \"WEEK_N\"]).agg(agg_dict).copy()\n",
        "    main_g.reset_index(col_level=1, inplace=True)\n",
        "    main_g = main_g.droplevel(level=0, axis=1)\n",
        "\n",
        "    # 표준편차 결측값 0으로 대치\n",
        "    main_g[\"P_STD\"].fillna(0, inplace=True)\n",
        "    \n",
        "    # 가격 중앙값 이동평균 \n",
        "    main_g[\"P_EMA5\"] = main_g[\"P_MEDIAN\"].ewm(5, min_periods=1).mean()\n",
        "    main_g[\"P_EMA15\"] = main_g[\"P_MEDIAN\"].ewm(15, min_periods=1).mean()\n",
        "    main_g[\"P_EMA30\"] = main_g[\"P_MEDIAN\"].ewm(30, min_periods=1).mean()\n",
        "    \n",
        "    # 거래량 이동평균\n",
        "    main_g[\"V_EMA5\"] = main_g[\"VOLUME\"].ewm(5, min_periods=1).mean()\n",
        "    main_g[\"V_EMA15\"] = main_g[\"VOLUME\"].ewm(15, min_periods=1).mean()\n",
        "    main_g[\"V_EMA30\"] = main_g[\"VOLUME\"].ewm(30, min_periods=1).mean()\n",
        "    \n",
        "    return main_g"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT7xLiljJdE8"
      },
      "source": [
        "def mmscale_loc(ds, target):\n",
        "    \"\"\"\n",
        "    target(y)과 x features를 나눠서 scale 하고,\n",
        "    scaler 모델을 저장한 후 반환함\n",
        "    \"\"\"\n",
        "    data = ds.copy()\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    mmscaler_x = MinMaxScaler()\n",
        "    mmscaler_y = MinMaxScaler()\n",
        "\n",
        "    # feature drop list\n",
        "    drop_list = [target]\n",
        "    if \"SANJI_NM\" in data.columns:\n",
        "        drop_list.append(\"SANJI_NM\")\n",
        "    if \"DELNG_DE\" in data.columns:\n",
        "        drop_list.append(\"DELNG_DE\")\n",
        "\n",
        "    # 변환할 컬럼 집합\n",
        "    scale_list = data.columns.drop(drop_list)\n",
        "    mmscaler_x = mmscaler_x.fit(data[scale_list])\n",
        "    mm_scaled_x = mmscaler_x.transform(data[scale_list])\n",
        "\n",
        "    mmscaler_y = mmscaler_y.fit(data[[target]])\n",
        "    mm_scaled_y = mmscaler_y.transform(data[[target]])\n",
        "\n",
        "    # 스케일된 값 to DataFrame\n",
        "    x_df = pd.DataFrame(mm_scaled_x, columns=scale_list)\n",
        "    y_df = pd.DataFrame(mm_scaled_y, columns=[target])\n",
        "\n",
        "    data_scaled = pd.concat([x_df, y_df] ,axis=1)\n",
        "\n",
        "    # 모델 저장\n",
        "    folder_path = FOLDER_PATH + \"Model/\"\n",
        "    x_path = f\"{folder_path}[MMS_X]{DEFAULT_FILE_NAME}\"\n",
        "    joblib.dump(mmscaler_x, f\"{x_path}.pkl\")\n",
        "    y_path = f\"{folder_path}[MMS_Y]{DEFAULT_FILE_NAME}\"\n",
        "    joblib.dump(mmscaler_y, f\"{y_path}.pkl\")\n",
        "\n",
        "    return (data_scaled, mmscaler_x, mmscaler_y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmgO4C45ZIIG"
      },
      "source": [
        "def change_datasets(ds, target, window_size=1, future_size=1):\n",
        "    \"\"\"\n",
        "    시계열 데이터로 새롭게 만들어진 데이터를\n",
        "    target과 train datasets으로 나누어 반환함\n",
        "    \"\"\"\n",
        "    datasets = ds.copy()\n",
        "\n",
        "    # 시계열 데이터 셋 생성 함수\n",
        "    def make_seq_dataset(x, y, window_size, future_size):\n",
        "        data = x.copy()\n",
        "        label = y.copy()\n",
        "        feature_list = []\n",
        "        label_list = []\n",
        "        # window_size 일간의 데이터 집합과 미래 future_size 후의 target 데이터로 나눔\n",
        "        for i in range(len(data) - window_size - future_size + 1):\n",
        "            # window_size 만틈의 데이터 집합\n",
        "            feature_list.append(\n",
        "                np.array(data.iloc[i:i+window_size])\n",
        "            )\n",
        "            # 미래 futre_size 후의 target 데이터\n",
        "            label_list.append(\n",
        "                np.array(label.iloc[i+window_size + future_size - 1])\n",
        "            )\n",
        "    \n",
        "        return np.array(feature_list), np.array(label_list)\n",
        "\n",
        "    # 시계열에 따라 나눠진 데이터를 target과 train datasets으로 나눔\n",
        "    x_col = datasets.columns.drop(target)\n",
        "    y_col = [target]\n",
        "\n",
        "    ds_x = datasets[x_col]\n",
        "    ds_y = datasets[y_col]\n",
        "\n",
        "    new_ds_x, new_ds_y = \\\n",
        "        make_seq_dataset(ds_x, ds_y, window_size, future_size)\n",
        "\n",
        "    return (new_ds_x, new_ds_y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYs4k6aDJdE_"
      },
      "source": [
        "def user_lstm(\n",
        "            train_ds, \n",
        "            location,\n",
        "            loss=\"mean_absolute_percentage_error\",\n",
        "            dropout=[0]\n",
        "        ):\n",
        "    \"\"\"\n",
        "    loss = [\"mean_absolute_percentage_error\", \"mean_absolute_error\", ...]\n",
        "    https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "    \"\"\"\n",
        "    train = train_ds.copy()\n",
        "\n",
        "    train_x, train_y = \\\n",
        "        change_datasets(train, TARGET, WINDOW_SIZE, FUTURE_SIZE)\n",
        "\n",
        "    x_train, x_valid, y_train, y_valid = \\\n",
        "        train_test_split(train_x, train_y, test_size=0.2)\n",
        "\n",
        "    # UNIT과 ACTIVE, DROPOUT 원소의 개수를 같게 줘야 함\n",
        "    unit_list = UNIT\n",
        "    active_list = ACTIVE\n",
        "    dropout_list = dropout\n",
        "    if not (len(unit_list) == len(active_list) == len(dropout_list)):\n",
        "        # 같지 않으면 마지막 값을 가장 큰 개수만큼 추가함\n",
        "        uad_len = [len(unit_list), len(active_list), len(dropout_list)]\n",
        "        max_len = max(uad_len)\n",
        "        uad_append_len = max_len - np.array(uad_len)\n",
        "        for i, append_n in enumerate(uad_append_len):\n",
        "            if append_n != 0:\n",
        "                if i == 0:\n",
        "                    for _ in range(append_n):\n",
        "                        unit_list.append(unit_list[uad_len[i] - 1])\n",
        "                elif i == 1:\n",
        "                    for _ in range(append_n):\n",
        "                        active_list.append(active_list[uad_len[i] - 1])\n",
        "                elif i == 2:\n",
        "                    for _ in range(append_n):\n",
        "                        dropout_list.append(dropout_list[uad_len[i] - 1])\n",
        "    else:\n",
        "        max_len = len(unit_list)\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # UNIT과 ACTIVE, DROPOUT, DENSE 중 가장 원소가 많은 수만큼 모델을 add함\n",
        "    for i in range(max_len):\n",
        "        # 마지막 반복이면 return_sequences는 False\n",
        "        if i == (max_len - 1):\n",
        "            return_sequences=False\n",
        "        else:\n",
        "            return_sequences=True\n",
        "\n",
        "        model.add(\n",
        "            LSTM(\n",
        "                unit_list[i],\n",
        "                input_shape=(train_x.shape[1], train_x.shape[2]),\n",
        "                activation=active_list[i],\n",
        "                return_sequences=return_sequences,\n",
        "            )\n",
        "        )\n",
        "        model.add(Dropout(dropout_list[i]))\n",
        "\n",
        "    model.add(Dense(1)) # Dense(Target 개수(종류))\n",
        "    \n",
        "    # 컴파일\n",
        "    model.compile(loss=loss, optimizer=\"adam\")\n",
        "\n",
        "    # 조기 중단 설정\n",
        "    ealry_stop = \\\n",
        "        EarlyStopping(\n",
        "            monitor=\"val_loss\", \n",
        "            patience=EARLYSTOPPOING # n개 이상 개선되지 않으면 중단\n",
        "        )\n",
        "\n",
        "    # 저장 관련 설정\n",
        "    folder_path = FOLDER_PATH + \"Model/\"\n",
        "    file_name = f\"[LSTM]{location}_{DEFAULT_FILE_NAME}\"\n",
        "    model_save_path = f\"{folder_path}{file_name}\"\n",
        "    \n",
        "    # 최적의 모델이 나올때마다 덮어쓰면서 저장됨\n",
        "    checkpoint = \\\n",
        "        ModelCheckpoint(\n",
        "            f\"{model_save_path}.h5\",\n",
        "            monitor=\"val_loss\",\n",
        "            verbose=1, # 0, 1\n",
        "            save_best_only=True, # 최적값일 때만 덮어씀(저장)\n",
        "            # save_weights_only=True, # weights만 저장, 전체 저장x\n",
        "            mode=\"auto\" # {max,min,auto}\n",
        "        )\n",
        "\n",
        "    history = \\\n",
        "        model.fit(\n",
        "            x_train, y_train,\n",
        "            epochs=EPOCH,\n",
        "            batch_size=BATCH,\n",
        "            validation_data=(x_valid, y_valid),\n",
        "            callbacks=[ealry_stop, checkpoint]\n",
        "    )\n",
        "\n",
        "    # save_model(model, f\"{model_save_path}.h5\")\n",
        "    # pickle로 모델 저장    \n",
        "    with open(f\"{model_save_path}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model,f)\n",
        "\n",
        "    evaluate = model.evaluate(x_valid, y_valid)\n",
        "\n",
        "    return model, history, evaluate, file_name"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcHIlBw_nHbU"
      },
      "source": [
        "def user_lstm_kfolded(\n",
        "                    train_ds, \n",
        "                    location,\n",
        "                    fold=5,\n",
        "                    loss=\"mean_absolute_percentage_error\",\n",
        "                    dropout=[0]\n",
        "                ):\n",
        "    \"\"\"\n",
        "    loss = [\"mean_absolute_percentage_error\", \"mean_absolute_error\", ...]\n",
        "    https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
        "    \"\"\"\n",
        "    train = train_ds.copy()\n",
        "    \n",
        "\n",
        "    train_x, train_y = \\\n",
        "        change_datasets(train, TARGET, WINDOW_SIZE, FUTURE_SIZE)\n",
        "\n",
        "    kf = KFold(n_splits=fold)\n",
        "\n",
        "    # UNIT과 ACTIVE, DROPOUT 원소의 개수를 같게 줘야 함\n",
        "    unit_list = UNIT\n",
        "    active_list = ACTIVE\n",
        "    dropout_list = dropout\n",
        "    if not (len(unit_list) == len(active_list) == len(dropout_list)):\n",
        "        # 같지 않으면 마지막 값을 가장 큰 개수만큼 추가함\n",
        "        uad_len = [len(unit_list), len(active_list), len(dropout_list)]\n",
        "        max_len = max(uad_len)\n",
        "        print(max_len)\n",
        "        uad_append_len = max_len - np.array(uad_len)\n",
        "        for i, append_n in enumerate(uad_append_len):\n",
        "            if append_n != 0:\n",
        "                if i == 0:\n",
        "                    for _ in range(append_n):\n",
        "                        unit_list.append(unit_list[uad_len[i] - 1])\n",
        "                elif i == 1:\n",
        "                    for _ in range(append_n):\n",
        "                        active_list.append(active_list[uad_len[i] - 1])\n",
        "                elif i == 2:\n",
        "                    for _ in range(append_n):\n",
        "                        dropout_list.append(dropout_list[uad_len[i] - 1])\n",
        "    else:\n",
        "        max_len = len(unit_list)\n",
        "\n",
        "\n",
        "    # 각 fold별 평가값이 저장될 리스트 생성\n",
        "    evaluate_list = []\n",
        "    model_list = []\n",
        "    # KFOLD 만큼 반복함\n",
        "    kn = 0\n",
        "    for train_i, valid_i in kf.split(train_x, train_y):    \n",
        "        model = Sequential()\n",
        "        kn += 1\n",
        "        # UNIT과 ACTIVE, DROPOUT, DENSE 중 가장 원소가 많은 수만큼 모델을 add함\n",
        "        for i in range(max_len):\n",
        "            # 마지막 반복이면 return_sequences는 False\n",
        "            if i == (max_len - 1):\n",
        "                return_sequences=False\n",
        "            else:\n",
        "                return_sequences=True\n",
        "\n",
        "            model.add(\n",
        "                LSTM(\n",
        "                    unit_list[i],\n",
        "                    input_shape=(train_x.shape[1], train_x.shape[2]),\n",
        "                    activation=active_list[i],\n",
        "                    return_sequences=return_sequences,\n",
        "                )\n",
        "            )\n",
        "            model.add(Dropout(dropout[i]))\n",
        "\n",
        "        model.add(Dense(1)) # Dense(Target 개수(종류))\n",
        "    \n",
        "        # 컴파일\n",
        "        model.compile(loss=loss, optimizer=\"adam\")\n",
        "\n",
        "        # 조기 중단 설정\n",
        "        ealry_stop = \\\n",
        "            EarlyStopping(\n",
        "                monitor=\"val_loss\", \n",
        "                patience=EARLYSTOPPOING # n개 이상 개선되지 않으면 중단\n",
        "            )\n",
        "\n",
        "        # 저장 관련 설정\n",
        "        folder_path = FOLDER_PATH + \"Model/\"\n",
        "        file_name = f\"[LSTM]{location}_{DEFAULT_FILE_NAME}_{kn}\"\n",
        "        model_save_path = f\"{folder_path}{file_name}\"\n",
        "        \n",
        "        # 최적의 모델이 나올때마다 덮어쓰면서 저장됨\n",
        "        checkpoint = \\\n",
        "            ModelCheckpoint(\n",
        "                f\"{model_save_path}.h5\",\n",
        "                monitor=\"val_loss\",\n",
        "                verbose=1, # 0, 1\n",
        "                save_best_only=True, # 최적값일 때만 덮어씀(저장)\n",
        "                # save_weights_only=True, # weights만 저장, 전체 저장x\n",
        "                mode=\"auto\" # {max,min,auto}\n",
        "            )\n",
        "\n",
        "        # valid 데이터셋 지정\n",
        "        valid_x = train_x[valid_i]\n",
        "        valid_y = train_y[valid_i]\n",
        "\n",
        "        history = \\\n",
        "            model.fit(\n",
        "                train_x[train_i], train_y[train_i],\n",
        "                epochs=EPOCH,\n",
        "                batch_size=BATCH,\n",
        "                validation_data=(valid_x, valid_y),\n",
        "                callbacks=[ealry_stop, checkpoint]\n",
        "        )\n",
        "        evaluate_list.append(model.evaluate(valid_x, valid_y))\n",
        "        model_list.append(model)\n",
        "    model_i = evaluate_list.index(min(evaluate_list))\n",
        "    \n",
        "    # 모델 저장\n",
        "    save_model(model_list(model_i), f\"{model_save_path}_f.h5\")\n",
        "    # pickle로 모델 저장    \n",
        "    with open(f\"{model_save_path}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model,f)\n",
        "\n",
        "    return model, history, mean(evaluate), file_name"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPsvblosCu01"
      },
      "source": [
        "def res_print(loc, path, model, x,y, mms_y, evaluate):\n",
        "\n",
        "    # RMSE\n",
        "    def rmse(actual, pred):\n",
        "        return sqrt(mean_squared_error(actual, pred))\n",
        "\n",
        "    # MAPE\n",
        "    def mape(actual, pred):\n",
        "        return np.mean(np.abs((actual - pred) / actual)) * 100\n",
        "\n",
        "    model.load_weights(f\"{FOLDER_PATH}Model/{path}.h5\")\n",
        "    pred = mms_y.inverse_transform(model.predict(x)).reshape(-1)\n",
        "    actual = mms_y.inverse_transform(y).reshape(-1)\n",
        "\n",
        "    pred_before_inv = model.predict(x).reshape(-1)\n",
        "    actual_before_inv = y.reshape(-1)\n",
        "\n",
        "    if pred.shape != actual.shape:\n",
        "        print(\"Prediction shape ERROR\")\n",
        "\n",
        "    rmse_raw = format(rmse(actual, pred), '20.5f')\n",
        "    rmse_scal = format(rmse(pred_before_inv, actual_before_inv), '20.5f')\n",
        "    mape = format(mape(actual, pred), '20.5f')\n",
        "    evaluate = format(evaluate, '20.5f')\n",
        "\n",
        "    text = (\n",
        "        f\"\"\"{'+' * 69}\\n\"\"\"\n",
        "        f\"\"\"location      = {loc}\\n\"\"\"\n",
        "        f\"\"\"week          = {WEEK}\\n\"\"\"\n",
        "        f\"\"\"target        = {TARGET}\\n\"\"\"\n",
        "        f\"\"\"future_size   = {FUTURE_SIZE}\\n\"\"\"\n",
        "        f\"\"\"window_size   = {WINDOW_SIZE}\\n\"\"\"\n",
        "        f\"\"\"test_size     = {TEST_SIZE}\\n\"\"\"\n",
        "        f\"\"\"unit          = {UNIT}\\n\"\"\"\n",
        "        f\"\"\"active        = {ACTIVE}\\n\"\"\"\n",
        "        f\"\"\"epoch         = {EPOCH}\\n\"\"\"\n",
        "        f\"\"\"batch         = {BATCH}\\n\"\"\"\n",
        "        f\"\"\"early stop    = {EARLYSTOPPOING}\\n\"\"\"\n",
        "        f\"\"\"dropout       = {DROPOUT}\\n\"\"\"\n",
        "        f\"\"\"model path    = {FOLDER_PATH}Model/{path}\\n\"\"\"\n",
        "        f\"\"\"{'+' * 69}\\n\"\"\"\n",
        "        f\"\"\"rmse(raw)     = {rmse_raw}\\n\"\"\"\n",
        "        f\"\"\"rmse(scaled)  = {rmse_scal}\\n\"\"\"\n",
        "        f\"\"\"MAPE          = {mape}\\n\"\"\"\n",
        "        f\"\"\"evaluate      = {evaluate}\\n\"\"\"\n",
        "        f\"\"\"{'+' * 69}\\n\"\"\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # 출력 결과 텍스트 파일로 저장\n",
        "    folder_path = FOLDER_PATH + \"Result/\"\n",
        "    res_save_path = f\"{folder_path}[R]{DEFAULT_FILE_NAME}.txt\"\n",
        "\n",
        "    f = open(res_save_path, 'a')\n",
        "    print(text, file=f)\n",
        "    f.close()\n",
        "    \n",
        "    # summary 저장\n",
        "    # https://pythonq.com/so/python/271129\n",
        "    from contextlib import redirect_stdout\n",
        "    with open(res_save_path, 'a') as f:\n",
        "        with redirect_stdout(f):\n",
        "            model.summary()\n",
        "\n",
        "    # 지역별 파일에 누적해서 결과 저장\n",
        "    res_save_path2 = f\"{folder_path}[{loc}]result_log.txt\"\n",
        "    f2 = open(res_save_path2, 'a')\n",
        "    print(text, file=f2)\n",
        "    f2.close()\n",
        "    \n",
        "    # summary 저장\n",
        "    # https://pythonq.com/so/python/271129\n",
        "    from contextlib import redirect_stdout\n",
        "    with open(res_save_path2, 'a') as f2:\n",
        "        with redirect_stdout(f2):\n",
        "            model.summary()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ITUSN_PCURx"
      },
      "source": [
        "def graph_loss(loc, history):\n",
        "    loss = history.history[\"loss\"]\n",
        "    val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "    sns.set_theme(\n",
        "        style=\"whitegrid\",\n",
        "        font=\"AppleGothic\", \n",
        "        rc={\"axes.unicode_minus\": False},\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.plot(\n",
        "        loss,\n",
        "        label=\"Train\",\n",
        "        color=\"navy\",\n",
        "        # alpha=0.6,\n",
        "    )\n",
        "    plt.plot(\n",
        "        val_loss,\n",
        "        label=\"Valid\",\n",
        "        color=\"darkgreen\",\n",
        "        # alpha=0.6,\n",
        "    )\n",
        "\n",
        "    # WEEK이면 W, DAY이면 D\n",
        "    date = 'W' if WEEK else 'D'\n",
        "\n",
        "    plt.title(\n",
        "        f\"{loc} MODEL LOSS({TARGET}, {date}+{FUTURE_SIZE})\",\n",
        "        pad=20,\n",
        "        fontsize=20,\n",
        "        fontweight=\"heavy\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "\n",
        "    plt.ylabel(\n",
        "        \"LOSS\",\n",
        "        fontsize=15,\n",
        "        fontweight=\"bold\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.xlabel(\n",
        "        \"EPOCH\",\n",
        "        fontsize=15,\n",
        "        fontweight=\"bold\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.xticks(\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.yticks(\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.legend(\n",
        "        loc=\"upper left\",\n",
        "        fontsize=15,\n",
        "        framealpha=0, # 배경색 투명도\n",
        "        ncol=2,\n",
        "    )\n",
        "    plt.grid(\n",
        "        True,\n",
        "        axis='y',\n",
        "        color='lightgray',\n",
        "        # alpha=0.8\n",
        "    )\n",
        "    plt.fill_between(\n",
        "        range(len(loss)), \n",
        "        loss, \n",
        "        val_loss, \n",
        "        color=\"lightpink\", \n",
        "        alpha=0.3,\n",
        "    )\n",
        "    plt.savefig(\n",
        "        f\"{FOLDER_PATH}Graph/[G][loss][{loc}]{DEFAULT_FILE_NAME}.png\",\n",
        "        transparent = True, # 배경색 투명하게\n",
        "        dpi=150, # 해상도\n",
        "    )\n",
        "    # plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDfceobolfir"
      },
      "source": [
        "def graph_predict(loc, model_path, model, x, y, mms_y):\n",
        "    model.load_weights(model_path)\n",
        "\n",
        "    # shape를 1D로 변환 후 inverse mmscale\n",
        "    pred = mms_y.inverse_transform(model.predict(x)).reshape(-1)\n",
        "    actual = mms_y.inverse_transform(y).reshape(-1)\n",
        "\n",
        "    # actual_pred_diff = actual - pred\n",
        "    # plus_diff_i = np.where(actual_pred_diff >= 0)[0]\n",
        "    # minus_diff_i = np.where(actual_pred_diff < 0)[0]\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.plot(\n",
        "        actual,\n",
        "        label=\"actual\",\n",
        "        color=\"navy\",\n",
        "        # alpha=0.6,\n",
        "    )\n",
        "    plt.plot(\n",
        "        pred,\n",
        "        label=\"Predict\",\n",
        "        color=\"darkgreen\",\n",
        "        # alpha=0.6,\n",
        "    )\n",
        "\n",
        "    # WEEK이면 W, DAY이면 D\n",
        "    date = 'W' if WEEK else 'D'\n",
        "\n",
        "    plt.title(\n",
        "        f\"{loc} LSTM Prediction({TARGET}, {date}+{FUTURE_SIZE})\",\n",
        "        pad=20,\n",
        "        fontsize=20,\n",
        "        fontweight=\"heavy\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.ylabel(\n",
        "        TARGET,\n",
        "        fontsize=15,\n",
        "        fontweight=\"bold\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.xlabel(\n",
        "        \"WEEK\",\n",
        "        fontsize=15,\n",
        "        fontweight=\"bold\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.yticks(\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.xticks(\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.legend(\n",
        "        loc=\"upper left\",\n",
        "        fontsize=15,\n",
        "        framealpha=0, # 배경색 투명도\n",
        "        ncol=2,\n",
        "    )\n",
        "    plt.grid(\n",
        "        True,\n",
        "        axis='y',\n",
        "        color='lightgray',\n",
        "        # alpha=0.8\n",
        "    )\n",
        "    plt.fill_between(\n",
        "        range(len(actual)), \n",
        "        pred, \n",
        "        actual, \n",
        "        color=\"lightpink\", \n",
        "        alpha=0.3\n",
        "    )\n",
        "    plt.savefig(\n",
        "        f\"{FOLDER_PATH}Graph/[G][predict][{loc}]{DEFAULT_FILE_NAME}.png\",\n",
        "        transparent = True, # 배경색 투명하게\n",
        "        dpi=150, # 해상도\n",
        "    )\n",
        "    # plt.show()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7x5bDwRMjyQ"
      },
      "source": [
        "def graph_train(loc, train_ds, model_path, model, mms_y):\n",
        "    \"\"\"\n",
        "    실제 값 예측 그래프\n",
        "    \"\"\"\n",
        "    train_x, train_y = \\\n",
        "        change_datasets(train_ds, TARGET, WINDOW_SIZE, FUTURE_SIZE)\n",
        "\n",
        "    model.load_weights(model_path)\n",
        "    # shape를 1D로 변환 후 inverse mmscale\n",
        "    pred = mms_y.inverse_transform(model.predict(train_x)).reshape(-1)\n",
        "    actual = mms_y.inverse_transform(train_y).reshape(-1)\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.plot(\n",
        "        actual,\n",
        "        label=\"actual\",\n",
        "        color=\"navy\",\n",
        "        # alpha=0.6,\n",
        "    )\n",
        "    plt.plot(\n",
        "        pred,\n",
        "        label=\"Predict\",\n",
        "        color=\"darkgreen\",\n",
        "        # alpha=0.6,\n",
        "    )\n",
        "\n",
        "    # WEEK이면 W, DAY이면 D\n",
        "    date = 'W' if WEEK else 'D'\n",
        "\n",
        "    plt.title(\n",
        "        f\"{loc} LSTM Train({TARGET}, {date}+{FUTURE_SIZE})\",\n",
        "        pad=20,\n",
        "        fontsize=20,\n",
        "        fontweight=\"heavy\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.ylabel(\n",
        "        TARGET,\n",
        "        fontsize=15,\n",
        "        fontweight=\"bold\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.xlabel(\n",
        "        \"WEEK\",\n",
        "        fontsize=15,\n",
        "        fontweight=\"bold\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.yticks(\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.xticks(\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.legend(\n",
        "        loc=\"upper left\",\n",
        "        fontsize=15,\n",
        "        framealpha=0, # 배경색 투명도\n",
        "        ncol=2,\n",
        "    )\n",
        "    plt.grid(\n",
        "        True,\n",
        "        axis='y',\n",
        "        color='lightgray',\n",
        "        # alpha=0.8\n",
        "    )\n",
        "    plt.fill_between(\n",
        "        range(len(actual)), \n",
        "        pred, \n",
        "        actual, \n",
        "        color=\"lightpink\", \n",
        "        alpha=0.3\n",
        "    )\n",
        "    plt.savefig(\n",
        "        f\"{FOLDER_PATH}Graph/[G][train][{loc}]{DEFAULT_FILE_NAME}.png\",\n",
        "        transparent = True, # 배경색 투명하게\n",
        "        dpi=150, # 해상도\n",
        "    )\n",
        "    # plt.show()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeCkwffHrmd_"
      },
      "source": [
        "def graph_finished(loc, scaled_ds, train_ds, model_path, model, mms_y, path):\n",
        "    \"\"\"\n",
        "    실제 및 예측 그래프\n",
        "    \"\"\"\n",
        "    train_x, train_y = \\\n",
        "        change_datasets(scaled_ds, TARGET, WINDOW_SIZE, FUTURE_SIZE)\n",
        "\n",
        "    model.load_weights(model_path)\n",
        "    # shape를 1D로 변환 후 inverse mmscale\n",
        "    pred = mms_y.inverse_transform(model.predict(train_x)).reshape(-1)\n",
        "    actual = mms_y.inverse_transform(train_y).reshape(-1)\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.plot(\n",
        "        actual,\n",
        "        label=\"actual\",\n",
        "        color=\"navy\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        pred,\n",
        "        label=\"Predict\",\n",
        "        color='darkgreen',\n",
        "    )\n",
        "\n",
        "    # WEEK이면 W, DAY이면 D\n",
        "    date = 'W' if WEEK else 'D' \n",
        "\n",
        "    plt.title(\n",
        "        f\"{loc} LSTM ({TARGET}, {date}+{FUTURE_SIZE})\",\n",
        "        pad=20,\n",
        "        fontsize=20,\n",
        "        fontweight=\"heavy\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.ylabel(\n",
        "        TARGET,\n",
        "        fontsize=15,\n",
        "        fontweight=\"bold\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.xlabel(\n",
        "        'WEEK',\n",
        "        fontsize=15,\n",
        "        fontweight=\"bold\",\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.yticks(\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.xticks(\n",
        "        color=\"gray\",\n",
        "    )\n",
        "    plt.legend(\n",
        "        loc=\"upper left\",\n",
        "        fontsize=15,\n",
        "        framealpha=0, # 배경색 투명도\n",
        "        ncol=2,\n",
        "    )\n",
        "    plt.grid(\n",
        "        True,\n",
        "        axis='y',\n",
        "        color='lightgray',\n",
        "        # alpha=0.8\n",
        "    )\n",
        "    # 그래프 사이 채움\n",
        "    plt.fill_between(\n",
        "        range(len(actual)), \n",
        "        pred, \n",
        "        actual, \n",
        "        color='lightpink', \n",
        "        alpha=0.3\n",
        "    )\n",
        "\n",
        "    # 예측이 시작되는 지점을 세로 선으로 구분함\n",
        "    line_n = change_datasets(train_ds, TARGET, WINDOW_SIZE)[0].shape[0]\n",
        "    plt.axvline(x=line_n, c='r', linestyle=\":\", linewidth=3)\n",
        "    plt.savefig(\n",
        "        f\"{FOLDER_PATH}Graph/[G][finished][{loc}]{path}.png\",\n",
        "        transparent = True, # 배경색 투명하게\n",
        "        dpi=150, # 해상도\n",
        "    )\n",
        "    # plt.show()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uubtgYoxnHbX",
        "outputId": "13231301-84e5-40fc-b03c-b1a2bbe68128"
      },
      "source": [
        "# 파일 경로 지정\n",
        "main_file_path = DEFAULT_PATH + \"지역_추출_도매_데이터_new_v3.0.csv\"\n",
        "weather_file_path = DEFAULT_PATH + \"[최종]기상데이터_2015_2021.csv\"\n",
        "\n",
        "# 파일 load\n",
        "main_raw = pd.read_csv(main_file_path, index_col=0)\n",
        "weather_raw = pd.read_csv(weather_file_path, index_col=0)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfWuIlmHJdE9"
      },
      "source": [
        "##############################################################################\n",
        "WEEK = True\n",
        "TARGET = \"VOLUME\"\n",
        "FUTURE_SIZE = 1 # 미래 n 단위기간 후 예측\n",
        "\n",
        "if WEEK:\n",
        "    # 1회 학습에 포함될 과거 N 단위 기간\n",
        "    WINDOW_SIZE = 20 # 약 5개월\n",
        "    # 테스트 데이터에 포함될 단위 기간 수\n",
        "    TEST_SIZE = 50 \n",
        "else:\n",
        "    # day\n",
        "    WINDOW_SIZE = 50\n",
        "    TEST_SIZE = 200\n",
        "\n",
        "LOCATION = [\"gangwon\", \"gyengnam\", \"gyeonggi\", \"jeonnam\"]\n",
        "# UNIT과 ACTIVE, DROPOUT 원소의 개수를 같게 줘야 함\n",
        "UNIT = [2 ** 8, 2 ** 8, 2 ** 7, 2 ** 7]\n",
        "ACTIVE = [\"softsign\", \"tanh\", \"tanh\", \"softsign\"] # \"relu\", \"tanh\" , \"elu\", \"softsign\"\n",
        "DROPOUT = [0, 0, 0, 0] # None or 0\n",
        "EPOCH = 400\n",
        "BATCH = 1\n",
        "EARLYSTOPPOING = 50\n",
        "\n",
        "##############################################################################\n",
        "FOLDER_PATH = DEFAULT_PATH + \"lstm_result/\"\n",
        "# 폴더 없으면 생성함\n",
        "# 생성할 폴더 리스트 [main, sub1, sub2...]\n",
        "SUB_FOLDER_LIST = ['', \"Model/\", \"Result/\", \"Graph/\"]\n",
        "for sub_f in SUB_FOLDER_LIST:\n",
        "    try:\n",
        "        if not os.path.exists(FOLDER_PATH + sub_f):\n",
        "            os.makedirs(FOLDER_PATH + sub_f)\n",
        "    except:\n",
        "        print(f\"Error: Do not create foler. {FOLDER_PATH + sub_f}\")\n",
        "\n",
        "SUFFIX_T = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
        "DEFAULT_FILE_NAME = \\\n",
        "    f\"\"\"{TARGET}\\\n",
        "    _WK_{WEEK}\\\n",
        "    _WS_{WINDOW_SIZE}\\\n",
        "    _FS_{FUTURE_SIZE}\\\n",
        "    _TS_{TEST_SIZE}\\\n",
        "    _U_{'+'.join(map(str, UNIT))}\\\n",
        "    _A_{'+'.join(map(str, ACTIVE))}\\\n",
        "    _E_{EPOCH}\\\n",
        "    _B_{BATCH}\\\n",
        "    _D_{'+'.join(map(str, DROPOUT))}\\\n",
        "    _{SUFFIX_T}\\\n",
        "    \"\"\".replace(' ', '')\n",
        "##############################################################################"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdKq_QbeJdE5"
      },
      "source": [
        "weather = pr_weather(weather_raw, week=WEEK)\n",
        "main = pr_main(main_raw, drop_leap_year=True)\n",
        "\n",
        "if WEEK:\n",
        "    main_g = main_to_week(main) \n",
        "    final_df = \\\n",
        "        pd.merge(\n",
        "            main_g, weather, \n",
        "            left_on=[\"YEAR\", \"WEEK_N\", \"SANJI_NM\"], \n",
        "            right_on=[\"YEAR\", \"WEEK_N\", \"관측지점명\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "    final_df.drop(columns=[\"관측지점명\", \"YEAR\", \"WEEK_N\"], inplace=True)\n",
        "else:\n",
        "    main_g = main_to_day(main)\n",
        "    final_df = \\\n",
        "        pd.merge(\n",
        "            main_g, weather, \n",
        "            left_on=[\"DELNG_DE\", \"SANJI_NM\"], \n",
        "            right_on=[\"관측시각\", \"관측지점명\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "    final_df.drop(columns=[\"관측지점명\", \"관측시각\"], inplace=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CVJL04LJdE8"
      },
      "source": [
        "gangwon = final_df[final_df[\"SANJI_NM\"] == \"강원도(평창+)\"].copy()\n",
        "gangwon.drop(columns=\"SANJI_NM\", inplace=True)\n",
        "jeonnam = final_df[final_df[\"SANJI_NM\"] == \"전라남도(진도+)\"].copy()\n",
        "jeonnam.drop(columns=\"SANJI_NM\", inplace=True)\n",
        "gyengnam = final_df[final_df[\"SANJI_NM\"] == \"경상남도(부산+)\"].copy()\n",
        "gyengnam.drop(columns=\"SANJI_NM\", inplace=True)\n",
        "gyeonggi = final_df[final_df[\"SANJI_NM\"] == \"경기도(구리+)\"].copy()\n",
        "gyeonggi.drop(columns=\"SANJI_NM\", inplace=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EVCGiJlJdE9"
      },
      "source": [
        "# mmsclaer\n",
        "data_dict = {\n",
        "    \"gangwon\": gangwon, \n",
        "    \"gyengnam\": gyengnam, \n",
        "    \"gyeonggi\": gyeonggi, \n",
        "    \"jeonnam\": jeonnam\n",
        "}\n",
        "for loc in LOCATION:\n",
        "    (\n",
        "        globals()[loc + \"_scaled\"], \n",
        "        globals()[loc + \"_mmsclaer_x\"], \n",
        "        globals()[loc + \"_mmsclaer_y\"], \n",
        "    ) = mmscale_loc(data_dict[loc], target=TARGET)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzLl-DxRhJsz"
      },
      "source": [
        "data_dict = {\n",
        "    \"gangwon\": gangwon_scaled, \n",
        "    \"gyengnam\": gyengnam_scaled, \n",
        "    \"gyeonggi\": gyeonggi_scaled, \n",
        "    \"jeonnam\": jeonnam_scaled\n",
        "}\n",
        "\n",
        "for loc in LOCATION:\n",
        "    globals()[\"train_\" + loc] = data_dict[loc][:-TEST_SIZE]\n",
        "    globals()[\"test_\" + loc] = data_dict[loc][-TEST_SIZE:]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rykkabqnHbZ"
      },
      "source": [
        "# # ### KFOLD\n",
        "# ##############################################################################\n",
        "# start_time = datetime.now()\n",
        "# ##############################################################################\n",
        "# train_dict = {\n",
        "#     \"gangwon\": train_gangwon, \n",
        "#     \"gyengnam\": train_gyengnam, \n",
        "#     \"gyeonggi\": train_gyeonggi, \n",
        "#     \"jeonnam\": train_jeonnam\n",
        "# }\n",
        "\n",
        "# test_dict = {\n",
        "#     \"gangwon\": test_gangwon, \n",
        "#     \"gyengnam\": test_gyengnam, \n",
        "#     \"gyeonggi\": test_gyeonggi, \n",
        "#     \"jeonnam\": test_jeonnam\n",
        "# }\n",
        "\n",
        "# for loc in LOCATION:\n",
        "#     (\n",
        "#         globals()[\"model_\" + loc], \n",
        "#         globals()[\"history_\" + loc], \n",
        "#         globals()[\"res_\" + loc],\n",
        "#         globals()[\"path_\" + loc],\n",
        "#      ) = \\\n",
        "#         user_lstm_kfolded(\n",
        "#             train_ds=data_dict[loc], \n",
        "#             location=loc,\n",
        "#             fold=2,\n",
        "#             dropout=DROPOUT,\n",
        "#         )\n",
        "#     globals()[\"test_x_\" + loc], globals()[\"test_y_\" + loc] = \\\n",
        "#         change_datasets(test_dict[loc], TARGET, WINDOW_SIZE, FUTURE_SIZE)\n",
        "              \n",
        "# ##############################################################################\n",
        "# end_time = datetime.now()\n",
        "# mm = (end_time - start_time).seconds // 60\n",
        "# ss = (end_time - start_time).seconds % 60\n",
        "# print(\"=\"*69)\n",
        "# print(f\"{mm}분 {ss}초\")\n",
        "# # ##############################################################################"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utp17y9iJdE_",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba24635f-d3a6-4a1a-d656-512636a85ec3"
      },
      "source": [
        "##############################################################################\n",
        "start_time = datetime.now()\n",
        "##############################################################################\n",
        "train_dict = {\n",
        "    \"gangwon\": train_gangwon, \n",
        "    \"gyengnam\": train_gyengnam, \n",
        "    \"gyeonggi\": train_gyeonggi, \n",
        "    \"jeonnam\": train_jeonnam\n",
        "}\n",
        "\n",
        "test_dict = {\n",
        "    \"gangwon\": test_gangwon, \n",
        "    \"gyengnam\": test_gyengnam, \n",
        "    \"gyeonggi\": test_gyeonggi, \n",
        "    \"jeonnam\": test_jeonnam\n",
        "}\n",
        "\n",
        "for loc in LOCATION:\n",
        "    (\n",
        "        globals()[\"model_\" + loc], \n",
        "        globals()[\"history_\" + loc], \n",
        "        globals()[\"res_\" + loc],\n",
        "        globals()[\"path_\" + loc],\n",
        "     ) = \\\n",
        "        user_lstm(\n",
        "            train_ds=train_dict[loc], \n",
        "            location=loc,\n",
        "            dropout=DROPOUT,\n",
        "        )\n",
        "    globals()[\"test_x_\" + loc], globals()[\"test_y_\" + loc] = \\\n",
        "        change_datasets(test_dict[loc], TARGET, WINDOW_SIZE, FUTURE_SIZE)\n",
        "              \n",
        "##############################################################################\n",
        "end_time = datetime.now()\n",
        "mm = (end_time - start_time).seconds // 60\n",
        "ss = (end_time - start_time).seconds % 60\n",
        "print(\"=\"*69)\n",
        "print(f\"{mm}분 {ss}초\")\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/400\n",
            "204/204 [==============================] - 22s 62ms/step - loss: 11243.2881 - val_loss: 258.4546\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 258.45456, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gangwon_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 2/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 796711.7500 - val_loss: 2377.9709\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 258.45456\n",
            "Epoch 3/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 369279.2500 - val_loss: 25479.4434\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 258.45456\n",
            "Epoch 4/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 13950507.0000 - val_loss: 447.6766\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 258.45456\n",
            "Epoch 5/400\n",
            "204/204 [==============================] - 11s 52ms/step - loss: 306601.5000 - val_loss: 8226.3145\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 258.45456\n",
            "Epoch 6/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 4816533.5000 - val_loss: 86.6419\n",
            "\n",
            "Epoch 00006: val_loss improved from 258.45456 to 86.64187, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gangwon_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 7/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 21856.4238 - val_loss: 1943.2758\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 86.64187\n",
            "Epoch 8/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 3238256.2500 - val_loss: 3649.1775\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 86.64187\n",
            "Epoch 9/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 1650147.6250 - val_loss: 2694.5505\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 86.64187\n",
            "Epoch 10/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 1454764.7500 - val_loss: 13157.6104\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 86.64187\n",
            "Epoch 11/400\n",
            "204/204 [==============================] - 11s 55ms/step - loss: 8162431.5000 - val_loss: 1036.4081\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 86.64187\n",
            "Epoch 12/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 654762.6875 - val_loss: 1700.0228\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 86.64187\n",
            "Epoch 13/400\n",
            "204/204 [==============================] - 11s 55ms/step - loss: 948761.1250 - val_loss: 432.3577\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 86.64187\n",
            "Epoch 14/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 284760.7500 - val_loss: 1213.8580\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 86.64187\n",
            "Epoch 15/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 660485.1250 - val_loss: 334.4262\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 86.64187\n",
            "Epoch 16/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 223328.9219 - val_loss: 981.5120\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 86.64187\n",
            "Epoch 17/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 522595.5625 - val_loss: 206.7003\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 86.64187\n",
            "Epoch 18/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 190793.6406 - val_loss: 754.4316\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 86.64187\n",
            "Epoch 19/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 388128.0312 - val_loss: 248.6659\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 86.64187\n",
            "Epoch 20/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 167210.6406 - val_loss: 579.2361\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 86.64187\n",
            "Epoch 21/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 284517.7812 - val_loss: 215.3781\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 86.64187\n",
            "Epoch 22/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 144544.7344 - val_loss: 505.1265\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 86.64187\n",
            "Epoch 23/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 240601.6094 - val_loss: 165.3299\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 86.64187\n",
            "Epoch 24/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 110739.2031 - val_loss: 409.4133\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 86.64187\n",
            "Epoch 25/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 183534.9219 - val_loss: 177.9948\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 86.64187\n",
            "Epoch 26/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 119342.2266 - val_loss: 309.4058\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 86.64187\n",
            "Epoch 27/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 124051.9297 - val_loss: 172.9478\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 86.64187\n",
            "Epoch 28/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 124189.6250 - val_loss: 241.9944\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 86.64187\n",
            "Epoch 29/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 84439.3359 - val_loss: 152.7943\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 86.64187\n",
            "Epoch 30/400\n",
            "204/204 [==============================] - 11s 55ms/step - loss: 101408.3594 - val_loss: 217.0845\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 86.64187\n",
            "Epoch 31/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 69566.5625 - val_loss: 134.3461\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 86.64187\n",
            "Epoch 32/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 90664.1484 - val_loss: 172.1269\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 86.64187\n",
            "Epoch 33/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 42730.4102 - val_loss: 137.2688\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 86.64187\n",
            "Epoch 34/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 90966.9531 - val_loss: 159.9656\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 86.64187\n",
            "Epoch 35/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 35704.4727 - val_loss: 125.4416\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 86.64187\n",
            "Epoch 36/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 81611.6172 - val_loss: 136.7686\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 86.64187\n",
            "Epoch 37/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 21933.5449 - val_loss: 117.6525\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 86.64187\n",
            "Epoch 38/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 74050.5234 - val_loss: 136.8238\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 86.64187\n",
            "Epoch 39/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 21896.4629 - val_loss: 112.4101\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 86.64187\n",
            "Epoch 40/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 69159.2422 - val_loss: 130.3699\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 86.64187\n",
            "Epoch 41/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 18135.2773 - val_loss: 108.1319\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 86.64187\n",
            "Epoch 42/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 64838.6328 - val_loss: 132.3540\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 86.64187\n",
            "Epoch 43/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 19287.0566 - val_loss: 103.7239\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 86.64187\n",
            "Epoch 44/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 59708.0508 - val_loss: 132.6096\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 86.64187\n",
            "Epoch 45/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 19454.0410 - val_loss: 101.1463\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 86.64187\n",
            "Epoch 46/400\n",
            "204/204 [==============================] - 11s 55ms/step - loss: 55959.1602 - val_loss: 133.9404\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 86.64187\n",
            "Epoch 47/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 24377.4922 - val_loss: 98.4953\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 86.64187\n",
            "Epoch 48/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 51448.8477 - val_loss: 139.2161\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 86.64187\n",
            "Epoch 49/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 23348.8926 - val_loss: 97.0742\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 86.64187\n",
            "Epoch 50/400\n",
            "204/204 [==============================] - 11s 53ms/step - loss: 48986.5703 - val_loss: 141.8858\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 86.64187\n",
            "Epoch 51/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 25315.5879 - val_loss: 107.7705\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 86.64187\n",
            "Epoch 52/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 50261.1484 - val_loss: 133.9034\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 86.64187\n",
            "Epoch 53/400\n",
            "204/204 [==============================] - 11s 54ms/step - loss: 20197.7324 - val_loss: 96.5618\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 86.64187\n",
            "Epoch 54/400\n",
            "204/204 [==============================] - 11s 55ms/step - loss: 48101.8438 - val_loss: 139.2034\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 86.64187\n",
            "Epoch 55/400\n",
            "204/204 [==============================] - 11s 55ms/step - loss: 23368.3164 - val_loss: 95.5025\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 86.64187\n",
            "Epoch 56/400\n",
            "204/204 [==============================] - 11s 55ms/step - loss: 46291.9375 - val_loss: 140.4015\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 86.64187\n",
            "2/2 [==============================] - 1s 24ms/step - loss: 140.4015\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/400\n",
            "205/205 [==============================] - 16s 60ms/step - loss: 43.2834 - val_loss: 13.5120\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 13.51202, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyengnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 2/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 35.9489 - val_loss: 14.3658\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 13.51202\n",
            "Epoch 3/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 39.1729 - val_loss: 17.8281\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 13.51202\n",
            "Epoch 4/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 37.3389 - val_loss: 15.1262\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 13.51202\n",
            "Epoch 5/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 35.7722 - val_loss: 14.1194\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 13.51202\n",
            "Epoch 6/400\n",
            "205/205 [==============================] - 11s 54ms/step - loss: 35.3837 - val_loss: 19.7419\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 13.51202\n",
            "Epoch 7/400\n",
            "205/205 [==============================] - 11s 54ms/step - loss: 35.2787 - val_loss: 14.4832\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 13.51202\n",
            "Epoch 8/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 33.9828 - val_loss: 13.3024\n",
            "\n",
            "Epoch 00008: val_loss improved from 13.51202 to 13.30244, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyengnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 9/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 35.1255 - val_loss: 13.8153\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 13.30244\n",
            "Epoch 10/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 34.3916 - val_loss: 13.4231\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 13.30244\n",
            "Epoch 11/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 33.9261 - val_loss: 17.6333\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 13.30244\n",
            "Epoch 12/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 34.7190 - val_loss: 14.9676\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 13.30244\n",
            "Epoch 13/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 34.6958 - val_loss: 13.4950\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 13.30244\n",
            "Epoch 14/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 33.3715 - val_loss: 44.6667\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 13.30244\n",
            "Epoch 15/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 35.3483 - val_loss: 13.8651\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 13.30244\n",
            "Epoch 16/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 34.3471 - val_loss: 13.3598\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 13.30244\n",
            "Epoch 17/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 34.8530 - val_loss: 13.3039\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 13.30244\n",
            "Epoch 18/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 34.1445 - val_loss: 15.3221\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 13.30244\n",
            "Epoch 19/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 33.8254 - val_loss: 15.0190\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 13.30244\n",
            "Epoch 20/400\n",
            "205/205 [==============================] - 12s 57ms/step - loss: 34.1818 - val_loss: 14.5180\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 13.30244\n",
            "Epoch 21/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 33.7937 - val_loss: 13.3253\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 13.30244\n",
            "Epoch 22/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 33.7070 - val_loss: 16.7207\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 13.30244\n",
            "Epoch 23/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 33.2211 - val_loss: 14.9874\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 13.30244\n",
            "Epoch 24/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 34.2515 - val_loss: 13.4499\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 13.30244\n",
            "Epoch 25/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 33.1345 - val_loss: 13.3044\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 13.30244\n",
            "Epoch 26/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 34.4031 - val_loss: 13.3232\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 13.30244\n",
            "Epoch 27/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 33.4848 - val_loss: 16.4875\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 13.30244\n",
            "Epoch 28/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 33.5189 - val_loss: 13.2920\n",
            "\n",
            "Epoch 00028: val_loss improved from 13.30244 to 13.29195, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyengnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 29/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 33.6343 - val_loss: 14.2231\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 13.29195\n",
            "Epoch 30/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 34.0141 - val_loss: 18.3336\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 13.29195\n",
            "Epoch 31/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 34.1022 - val_loss: 13.2947\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 13.29195\n",
            "Epoch 32/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 32.1791 - val_loss: 19.8550\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 13.29195\n",
            "Epoch 33/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 33.4559 - val_loss: 13.3293\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 13.29195\n",
            "Epoch 34/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 33.6151 - val_loss: 14.3065\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 13.29195\n",
            "Epoch 35/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 33.4627 - val_loss: 15.1815\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 13.29195\n",
            "Epoch 36/400\n",
            "205/205 [==============================] - 12s 57ms/step - loss: 34.4270 - val_loss: 13.9805\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 13.29195\n",
            "Epoch 37/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 34.4033 - val_loss: 13.7840\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 13.29195\n",
            "Epoch 38/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 33.3007 - val_loss: 13.8368\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 13.29195\n",
            "Epoch 39/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 33.5594 - val_loss: 13.9645\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 13.29195\n",
            "Epoch 40/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 32.7806 - val_loss: 13.4868\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 13.29195\n",
            "Epoch 41/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 31.7144 - val_loss: 29.7462\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 13.29195\n",
            "Epoch 42/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 32.2555 - val_loss: 14.3639\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 13.29195\n",
            "Epoch 43/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 31.1940 - val_loss: 15.2058\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 13.29195\n",
            "Epoch 44/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 30.5999 - val_loss: 14.8384\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 13.29195\n",
            "Epoch 45/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 30.3744 - val_loss: 14.4278\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 13.29195\n",
            "Epoch 46/400\n",
            "205/205 [==============================] - 12s 57ms/step - loss: 28.3023 - val_loss: 20.3121\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 13.29195\n",
            "Epoch 47/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 30.7185 - val_loss: 17.5747\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 13.29195\n",
            "Epoch 48/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 29.0290 - val_loss: 14.3120\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 13.29195\n",
            "Epoch 49/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 29.0653 - val_loss: 16.6785\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 13.29195\n",
            "Epoch 50/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 25.6011 - val_loss: 15.0614\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 13.29195\n",
            "Epoch 51/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 23.8011 - val_loss: 18.2128\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 13.29195\n",
            "Epoch 52/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 20.8450 - val_loss: 14.7465\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 13.29195\n",
            "Epoch 53/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 20.6886 - val_loss: 16.8263\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 13.29195\n",
            "Epoch 54/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 19.7670 - val_loss: 18.6769\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 13.29195\n",
            "Epoch 55/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 18.9925 - val_loss: 17.9426\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 13.29195\n",
            "Epoch 56/400\n",
            "205/205 [==============================] - 11s 55ms/step - loss: 21.8472 - val_loss: 23.2422\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 13.29195\n",
            "Epoch 57/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 19.7415 - val_loss: 22.4752\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 13.29195\n",
            "Epoch 58/400\n",
            "205/205 [==============================] - 12s 57ms/step - loss: 19.1611 - val_loss: 17.3683\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 13.29195\n",
            "Epoch 59/400\n",
            "205/205 [==============================] - 12s 57ms/step - loss: 18.7807 - val_loss: 14.5035\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 13.29195\n",
            "Epoch 60/400\n",
            "205/205 [==============================] - 12s 57ms/step - loss: 16.3767 - val_loss: 16.4942\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 13.29195\n",
            "Epoch 61/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 15.4693 - val_loss: 15.4125\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 13.29195\n",
            "Epoch 62/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 14.6017 - val_loss: 17.9314\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 13.29195\n",
            "Epoch 63/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 15.3815 - val_loss: 19.2108\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 13.29195\n",
            "Epoch 64/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 15.4634 - val_loss: 16.2591\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 13.29195\n",
            "Epoch 65/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 14.3448 - val_loss: 15.9865\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 13.29195\n",
            "Epoch 66/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 26.2977 - val_loss: 19.9666\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 13.29195\n",
            "Epoch 67/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 23.5863 - val_loss: 17.5959\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 13.29195\n",
            "Epoch 68/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 16.4343 - val_loss: 19.2218\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 13.29195\n",
            "Epoch 69/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 15.1825 - val_loss: 18.5497\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 13.29195\n",
            "Epoch 70/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 15.2965 - val_loss: 16.6091\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 13.29195\n",
            "Epoch 71/400\n",
            "205/205 [==============================] - 11s 56ms/step - loss: 14.4346 - val_loss: 17.1917\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 13.29195\n",
            "Epoch 72/400\n",
            "205/205 [==============================] - 12s 57ms/step - loss: 13.3278 - val_loss: 17.7314\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 13.29195\n",
            "Epoch 73/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 13.3478 - val_loss: 15.6462\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 13.29195\n",
            "Epoch 74/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 13.1616 - val_loss: 15.9061\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 13.29195\n",
            "Epoch 75/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 11.9669 - val_loss: 16.7537\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 13.29195\n",
            "Epoch 76/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 12.8097 - val_loss: 16.8255\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 13.29195\n",
            "Epoch 77/400\n",
            "205/205 [==============================] - 12s 56ms/step - loss: 12.5224 - val_loss: 18.4341\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 13.29195\n",
            "Epoch 78/400\n",
            "205/205 [==============================] - 12s 57ms/step - loss: 12.9002 - val_loss: 14.5123\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 13.29195\n",
            "2/2 [==============================] - 1s 25ms/step - loss: 14.5123\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/400\n",
            "204/204 [==============================] - 17s 62ms/step - loss: 75.5086 - val_loss: 40.4442\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 40.44424, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyeonggi_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 2/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 48.6313 - val_loss: 40.9873\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 40.44424\n",
            "Epoch 3/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 59.9228 - val_loss: 44.6723\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 40.44424\n",
            "Epoch 4/400\n",
            "204/204 [==============================] - 11s 56ms/step - loss: 49.7049 - val_loss: 30.8152\n",
            "\n",
            "Epoch 00004: val_loss improved from 40.44424 to 30.81518, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyeonggi_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 5/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 39.3208 - val_loss: 22.7975\n",
            "\n",
            "Epoch 00005: val_loss improved from 30.81518 to 22.79750, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyeonggi_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 6/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 41.7714 - val_loss: 28.3658\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 22.79750\n",
            "Epoch 7/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 44.4785 - val_loss: 31.0021\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 22.79750\n",
            "Epoch 8/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 40.9854 - val_loss: 36.6312\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 22.79750\n",
            "Epoch 9/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 43.0783 - val_loss: 29.8584\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 22.79750\n",
            "Epoch 10/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 35.3586 - val_loss: 25.0411\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 22.79750\n",
            "Epoch 11/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 45.6056 - val_loss: 28.9538\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 22.79750\n",
            "Epoch 12/400\n",
            "204/204 [==============================] - 12s 56ms/step - loss: 34.5147 - val_loss: 21.6427\n",
            "\n",
            "Epoch 00012: val_loss improved from 22.79750 to 21.64270, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyeonggi_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 13/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 36.9954 - val_loss: 29.4162\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 21.64270\n",
            "Epoch 14/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 43.7736 - val_loss: 41.8417\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 21.64270\n",
            "Epoch 15/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 45.0681 - val_loss: 41.9141\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 21.64270\n",
            "Epoch 16/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 42.4325 - val_loss: 52.5299\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 21.64270\n",
            "Epoch 17/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 40.4692 - val_loss: 30.2269\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 21.64270\n",
            "Epoch 18/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 36.2166 - val_loss: 27.8910\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 21.64270\n",
            "Epoch 19/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 38.9860 - val_loss: 35.1119\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 21.64270\n",
            "Epoch 20/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 43.0186 - val_loss: 27.2308\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 21.64270\n",
            "Epoch 21/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 41.2968 - val_loss: 28.3664\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 21.64270\n",
            "Epoch 22/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 42.8540 - val_loss: 25.4328\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 21.64270\n",
            "Epoch 23/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 35.8662 - val_loss: 29.5000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 21.64270\n",
            "Epoch 24/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 36.0130 - val_loss: 24.9043\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 21.64270\n",
            "Epoch 25/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 30.1284 - val_loss: 26.5661\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 21.64270\n",
            "Epoch 26/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 32.1589 - val_loss: 26.5312\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 21.64270\n",
            "Epoch 27/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 27.9768 - val_loss: 27.7894\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 21.64270\n",
            "Epoch 28/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 29.2920 - val_loss: 25.4016\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 21.64270\n",
            "Epoch 29/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 29.2495 - val_loss: 23.4161\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 21.64270\n",
            "Epoch 30/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 27.4157 - val_loss: 34.4216\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 21.64270\n",
            "Epoch 31/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 28.1881 - val_loss: 28.0112\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 21.64270\n",
            "Epoch 32/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 26.9877 - val_loss: 22.6794\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 21.64270\n",
            "Epoch 33/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 26.8510 - val_loss: 22.2975\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 21.64270\n",
            "Epoch 34/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 26.7526 - val_loss: 23.3631\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 21.64270\n",
            "Epoch 35/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 25.3969 - val_loss: 19.4358\n",
            "\n",
            "Epoch 00035: val_loss improved from 21.64270 to 19.43580, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyeonggi_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 36/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 32.6542 - val_loss: 21.1479\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 19.43580\n",
            "Epoch 37/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 33.4953 - val_loss: 24.8137\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 19.43580\n",
            "Epoch 38/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 29.1062 - val_loss: 25.8519\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 19.43580\n",
            "Epoch 39/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 29.3506 - val_loss: 21.0839\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 19.43580\n",
            "Epoch 40/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 27.2306 - val_loss: 23.8367\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 19.43580\n",
            "Epoch 41/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 23.7269 - val_loss: 20.8081\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 19.43580\n",
            "Epoch 42/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 47.9852 - val_loss: 38.3813\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 19.43580\n",
            "Epoch 43/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 30.8687 - val_loss: 26.0344\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 19.43580\n",
            "Epoch 44/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 25.2289 - val_loss: 21.1582\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 19.43580\n",
            "Epoch 45/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 29.4380 - val_loss: 21.2964\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 19.43580\n",
            "Epoch 46/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 25.2466 - val_loss: 22.7742\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 19.43580\n",
            "Epoch 47/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 24.6903 - val_loss: 21.4941\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 19.43580\n",
            "Epoch 48/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 24.7255 - val_loss: 19.6737\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 19.43580\n",
            "Epoch 49/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 25.2716 - val_loss: 23.5258\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 19.43580\n",
            "Epoch 50/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 25.5401 - val_loss: 23.2926\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 19.43580\n",
            "Epoch 51/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 23.4813 - val_loss: 28.6997\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 19.43580\n",
            "Epoch 52/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 29.4315 - val_loss: 20.0799\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 19.43580\n",
            "Epoch 53/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 24.2105 - val_loss: 20.7499\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 19.43580\n",
            "Epoch 54/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 23.3658 - val_loss: 20.5570\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 19.43580\n",
            "Epoch 55/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 23.0828 - val_loss: 21.3988\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 19.43580\n",
            "Epoch 56/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 24.9063 - val_loss: 22.2575\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 19.43580\n",
            "Epoch 57/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 22.1005 - val_loss: 21.8789\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 19.43580\n",
            "Epoch 58/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 25.3781 - val_loss: 19.3938\n",
            "\n",
            "Epoch 00058: val_loss improved from 19.43580 to 19.39375, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyeonggi_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 59/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 24.0320 - val_loss: 20.6111\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 19.39375\n",
            "Epoch 60/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 25.6124 - val_loss: 20.4084\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 19.39375\n",
            "Epoch 61/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 24.9855 - val_loss: 22.0346\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 19.39375\n",
            "Epoch 62/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 30.2327 - val_loss: 23.3347\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 19.39375\n",
            "Epoch 63/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 35.0612 - val_loss: 21.3848\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 19.39375\n",
            "Epoch 64/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 33.2257 - val_loss: 18.7419\n",
            "\n",
            "Epoch 00064: val_loss improved from 19.39375 to 18.74188, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyeonggi_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 65/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 31.1404 - val_loss: 21.5530\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 18.74188\n",
            "Epoch 66/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 33.8427 - val_loss: 22.4725\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 18.74188\n",
            "Epoch 67/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 31.5781 - val_loss: 23.2923\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 18.74188\n",
            "Epoch 68/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 61.0469 - val_loss: 37.5892\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 18.74188\n",
            "Epoch 69/400\n",
            "204/204 [==============================] - 12s 57ms/step - loss: 43.6639 - val_loss: 40.2715\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 18.74188\n",
            "Epoch 70/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 30.0869 - val_loss: 23.1926\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 18.74188\n",
            "Epoch 71/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 24.7993 - val_loss: 20.9470\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 18.74188\n",
            "Epoch 72/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 24.9148 - val_loss: 19.3931\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 18.74188\n",
            "Epoch 73/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 28.9085 - val_loss: 17.7898\n",
            "\n",
            "Epoch 00073: val_loss improved from 18.74188 to 17.78984, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyeonggi_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 74/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 26.3396 - val_loss: 20.8698\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 17.78984\n",
            "Epoch 75/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 25.1718 - val_loss: 19.1806\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 17.78984\n",
            "Epoch 76/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 24.4002 - val_loss: 18.6584\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 17.78984\n",
            "Epoch 77/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 22.9882 - val_loss: 20.7277\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 17.78984\n",
            "Epoch 78/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 22.2369 - val_loss: 24.1387\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 17.78984\n",
            "Epoch 79/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 23.6315 - val_loss: 22.1693\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 17.78984\n",
            "Epoch 80/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 21.8638 - val_loss: 26.9066\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 17.78984\n",
            "Epoch 81/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 20.2885 - val_loss: 18.5747\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 17.78984\n",
            "Epoch 82/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 20.6014 - val_loss: 26.4601\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 17.78984\n",
            "Epoch 83/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 21.2522 - val_loss: 25.3318\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 17.78984\n",
            "Epoch 84/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 21.6550 - val_loss: 19.0602\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 17.78984\n",
            "Epoch 85/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 23.5344 - val_loss: 19.3592\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 17.78984\n",
            "Epoch 86/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 24.6203 - val_loss: 18.5364\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 17.78984\n",
            "Epoch 87/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 21.9513 - val_loss: 21.9261\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 17.78984\n",
            "Epoch 88/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 19.5068 - val_loss: 24.0282\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 17.78984\n",
            "Epoch 89/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 20.5112 - val_loss: 17.1160\n",
            "\n",
            "Epoch 00089: val_loss improved from 17.78984 to 17.11601, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]gyeonggi_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 90/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 23.1961 - val_loss: 18.4059\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 17.11601\n",
            "Epoch 91/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 19.7065 - val_loss: 20.4875\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 17.11601\n",
            "Epoch 92/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 19.3298 - val_loss: 19.1523\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 17.11601\n",
            "Epoch 93/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 19.6293 - val_loss: 18.6608\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 17.11601\n",
            "Epoch 94/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 21.3123 - val_loss: 18.5565\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 17.11601\n",
            "Epoch 95/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 20.6774 - val_loss: 23.8394\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 17.11601\n",
            "Epoch 96/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 18.4537 - val_loss: 19.0364\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 17.11601\n",
            "Epoch 97/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 18.9436 - val_loss: 20.2240\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 17.11601\n",
            "Epoch 98/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 19.6739 - val_loss: 20.2982\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 17.11601\n",
            "Epoch 99/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 20.6601 - val_loss: 19.3388\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 17.11601\n",
            "Epoch 100/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 19.3290 - val_loss: 18.3078\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 17.11601\n",
            "Epoch 101/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 17.8774 - val_loss: 21.0059\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 17.11601\n",
            "Epoch 102/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 19.1575 - val_loss: 19.1586\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 17.11601\n",
            "Epoch 103/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 19.1319 - val_loss: 20.2781\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 17.11601\n",
            "Epoch 104/400\n",
            "204/204 [==============================] - 12s 58ms/step - loss: 19.2870 - val_loss: 18.8303\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 17.11601\n",
            "Epoch 105/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 19.2827 - val_loss: 22.0164\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 17.11601\n",
            "Epoch 106/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 19.5745 - val_loss: 23.4219\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 17.11601\n",
            "Epoch 107/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 20.7045 - val_loss: 22.8080\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 17.11601\n",
            "Epoch 108/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 23.9185 - val_loss: 19.6006\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 17.11601\n",
            "Epoch 109/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 19.0263 - val_loss: 20.6913\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 17.11601\n",
            "Epoch 110/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 18.0100 - val_loss: 21.1617\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 17.11601\n",
            "Epoch 111/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 17.5535 - val_loss: 22.9625\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 17.11601\n",
            "Epoch 112/400\n",
            "204/204 [==============================] - 12s 61ms/step - loss: 19.4075 - val_loss: 23.8031\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 17.11601\n",
            "Epoch 113/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 18.0640 - val_loss: 19.2119\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 17.11601\n",
            "Epoch 114/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 18.6769 - val_loss: 20.9919\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 17.11601\n",
            "Epoch 115/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 21.8738 - val_loss: 22.4386\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 17.11601\n",
            "Epoch 116/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 28.4594 - val_loss: 25.4194\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 17.11601\n",
            "Epoch 117/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 22.7897 - val_loss: 24.2578\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 17.11601\n",
            "Epoch 118/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 19.9039 - val_loss: 23.1794\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 17.11601\n",
            "Epoch 119/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 19.2534 - val_loss: 20.2451\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 17.11601\n",
            "Epoch 120/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 19.1033 - val_loss: 20.7876\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 17.11601\n",
            "Epoch 121/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 17.7659 - val_loss: 23.7691\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 17.11601\n",
            "Epoch 122/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 18.3862 - val_loss: 23.9306\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 17.11601\n",
            "Epoch 123/400\n",
            "204/204 [==============================] - 12s 59ms/step - loss: 18.2582 - val_loss: 21.1628\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 17.11601\n",
            "Epoch 124/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 16.2516 - val_loss: 20.4062\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 17.11601\n",
            "Epoch 125/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 16.1763 - val_loss: 21.9318\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 17.11601\n",
            "Epoch 126/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 16.7308 - val_loss: 24.3036\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 17.11601\n",
            "Epoch 127/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 15.8715 - val_loss: 27.6312\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 17.11601\n",
            "Epoch 128/400\n",
            "204/204 [==============================] - 12s 61ms/step - loss: 16.9817 - val_loss: 20.8986\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 17.11601\n",
            "Epoch 129/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 15.6672 - val_loss: 22.5643\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 17.11601\n",
            "Epoch 130/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 16.7232 - val_loss: 19.3954\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 17.11601\n",
            "Epoch 131/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 15.1637 - val_loss: 21.9033\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 17.11601\n",
            "Epoch 132/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 16.3198 - val_loss: 22.1774\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 17.11601\n",
            "Epoch 133/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 16.2906 - val_loss: 26.1378\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 17.11601\n",
            "Epoch 134/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 16.1743 - val_loss: 20.9924\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 17.11601\n",
            "Epoch 135/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 15.2731 - val_loss: 21.6973\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 17.11601\n",
            "Epoch 136/400\n",
            "204/204 [==============================] - 12s 60ms/step - loss: 16.2844 - val_loss: 23.4245\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 17.11601\n",
            "Epoch 137/400\n",
            "204/204 [==============================] - 12s 61ms/step - loss: 15.7141 - val_loss: 21.3857\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 17.11601\n",
            "Epoch 138/400\n",
            "204/204 [==============================] - 12s 61ms/step - loss: 16.1748 - val_loss: 21.5540\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 17.11601\n",
            "Epoch 139/400\n",
            "204/204 [==============================] - 12s 61ms/step - loss: 16.1684 - val_loss: 23.2792\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 17.11601\n",
            "2/2 [==============================] - 1s 29ms/step - loss: 23.2792\n",
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/400\n",
            "205/205 [==============================] - 18s 66ms/step - loss: 106.5917 - val_loss: 62.0034\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 62.00341, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 2/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 78.1492 - val_loss: 64.6663\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 62.00341\n",
            "Epoch 3/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 77.2244 - val_loss: 54.9273\n",
            "\n",
            "Epoch 00003: val_loss improved from 62.00341 to 54.92730, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 4/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 69.6125 - val_loss: 59.4128\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 54.92730\n",
            "Epoch 5/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 67.3062 - val_loss: 69.9720\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 54.92730\n",
            "Epoch 6/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 67.1414 - val_loss: 54.9189\n",
            "\n",
            "Epoch 00006: val_loss improved from 54.92730 to 54.91892, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 7/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 64.8090 - val_loss: 64.8119\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 54.91892\n",
            "Epoch 8/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 64.7171 - val_loss: 83.8015\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 54.91892\n",
            "Epoch 9/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 64.8196 - val_loss: 54.2135\n",
            "\n",
            "Epoch 00009: val_loss improved from 54.91892 to 54.21351, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 10/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 70.7310 - val_loss: 66.8047\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 54.21351\n",
            "Epoch 11/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 63.8693 - val_loss: 55.7112\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 54.21351\n",
            "Epoch 12/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 65.6067 - val_loss: 53.3339\n",
            "\n",
            "Epoch 00012: val_loss improved from 54.21351 to 53.33388, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 13/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 64.2012 - val_loss: 46.9223\n",
            "\n",
            "Epoch 00013: val_loss improved from 53.33388 to 46.92234, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 14/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 68.0329 - val_loss: 55.3689\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 46.92234\n",
            "Epoch 15/400\n",
            "205/205 [==============================] - 12s 60ms/step - loss: 64.6592 - val_loss: 52.4405\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 46.92234\n",
            "Epoch 16/400\n",
            "205/205 [==============================] - 12s 60ms/step - loss: 52.3753 - val_loss: 50.9512\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 46.92234\n",
            "Epoch 17/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 91.4283 - val_loss: 36.9190\n",
            "\n",
            "Epoch 00017: val_loss improved from 46.92234 to 36.91904, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 18/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 94.7880 - val_loss: 51.3353\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 36.91904\n",
            "Epoch 19/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 98.6326 - val_loss: 41.6596\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 36.91904\n",
            "Epoch 20/400\n",
            "205/205 [==============================] - 12s 60ms/step - loss: 89.2956 - val_loss: 56.4148\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 36.91904\n",
            "Epoch 21/400\n",
            "205/205 [==============================] - 12s 60ms/step - loss: 66.6700 - val_loss: 54.7938\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 36.91904\n",
            "Epoch 22/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 65.1762 - val_loss: 58.7675\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 36.91904\n",
            "Epoch 23/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 63.3181 - val_loss: 55.7744\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 36.91904\n",
            "Epoch 24/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 64.3264 - val_loss: 59.9255\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 36.91904\n",
            "Epoch 25/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 62.0239 - val_loss: 55.7391\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 36.91904\n",
            "Epoch 26/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 61.5241 - val_loss: 57.9483\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 36.91904\n",
            "Epoch 27/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 61.8158 - val_loss: 57.6447\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 36.91904\n",
            "Epoch 28/400\n",
            "205/205 [==============================] - 12s 60ms/step - loss: 58.5192 - val_loss: 45.6887\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 36.91904\n",
            "Epoch 29/400\n",
            "205/205 [==============================] - 12s 60ms/step - loss: 57.6032 - val_loss: 60.4944\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 36.91904\n",
            "Epoch 30/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 57.1895 - val_loss: 47.8604\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 36.91904\n",
            "Epoch 31/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 59.4863 - val_loss: 57.5483\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 36.91904\n",
            "Epoch 32/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 62.3264 - val_loss: 53.5751\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 36.91904\n",
            "Epoch 33/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 53.3750 - val_loss: 38.7641\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 36.91904\n",
            "Epoch 34/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 61.9742 - val_loss: 50.8242\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 36.91904\n",
            "Epoch 35/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 44.9036 - val_loss: 40.1361\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 36.91904\n",
            "Epoch 36/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 39.1247 - val_loss: 28.8739\n",
            "\n",
            "Epoch 00036: val_loss improved from 36.91904 to 28.87393, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 37/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 70.7804 - val_loss: 45.9374\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 28.87393\n",
            "Epoch 38/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 71.3085 - val_loss: 53.5600\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 28.87393\n",
            "Epoch 39/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 61.6037 - val_loss: 56.8994\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 28.87393\n",
            "Epoch 40/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 54.4501 - val_loss: 57.2343\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 28.87393\n",
            "Epoch 41/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 57.0051 - val_loss: 45.6518\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 28.87393\n",
            "Epoch 42/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 50.0805 - val_loss: 35.6765\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 28.87393\n",
            "Epoch 43/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 57.7175 - val_loss: 56.8182\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 28.87393\n",
            "Epoch 44/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 63.1332 - val_loss: 53.3889\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 28.87393\n",
            "Epoch 45/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 60.4142 - val_loss: 57.8320\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 28.87393\n",
            "Epoch 46/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 71.7083 - val_loss: 59.5244\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 28.87393\n",
            "Epoch 47/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 63.0637 - val_loss: 51.7999\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 28.87393\n",
            "Epoch 48/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 54.2894 - val_loss: 41.1514\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 28.87393\n",
            "Epoch 49/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 44.7479 - val_loss: 36.5347\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 28.87393\n",
            "Epoch 50/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 67.3871 - val_loss: 38.2117\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 28.87393\n",
            "Epoch 51/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 43.7917 - val_loss: 37.3769\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 28.87393\n",
            "Epoch 52/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 42.4278 - val_loss: 34.4468\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 28.87393\n",
            "Epoch 53/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 36.6585 - val_loss: 32.6809\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 28.87393\n",
            "Epoch 54/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 36.8981 - val_loss: 32.8389\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 28.87393\n",
            "Epoch 55/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 54.0387 - val_loss: 52.2751\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 28.87393\n",
            "Epoch 56/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 59.2938 - val_loss: 40.1187\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 28.87393\n",
            "Epoch 57/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 41.2077 - val_loss: 37.7762\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 28.87393\n",
            "Epoch 58/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 36.2632 - val_loss: 37.1077\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 28.87393\n",
            "Epoch 59/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 38.4534 - val_loss: 29.5597\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 28.87393\n",
            "Epoch 60/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 35.9849 - val_loss: 34.1610\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 28.87393\n",
            "Epoch 61/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 32.7279 - val_loss: 41.7965\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 28.87393\n",
            "Epoch 62/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 63.1665 - val_loss: 51.1912\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 28.87393\n",
            "Epoch 63/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 38.7973 - val_loss: 29.6684\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 28.87393\n",
            "Epoch 64/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 35.7988 - val_loss: 28.1019\n",
            "\n",
            "Epoch 00064: val_loss improved from 28.87393 to 28.10186, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 65/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 32.2207 - val_loss: 31.0618\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 28.10186\n",
            "Epoch 66/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 30.8688 - val_loss: 26.5070\n",
            "\n",
            "Epoch 00066: val_loss improved from 28.10186 to 26.50703, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 67/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 60.7646 - val_loss: 40.5644\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 26.50703\n",
            "Epoch 68/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 38.1929 - val_loss: 31.3748\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 26.50703\n",
            "Epoch 69/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 33.5574 - val_loss: 129.8390\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 26.50703\n",
            "Epoch 70/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 49.0423 - val_loss: 33.4506\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 26.50703\n",
            "Epoch 71/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 36.0497 - val_loss: 29.1899\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 26.50703\n",
            "Epoch 72/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 35.3349 - val_loss: 30.7177\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 26.50703\n",
            "Epoch 73/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 33.7339 - val_loss: 33.5672\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 26.50703\n",
            "Epoch 74/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 29.9218 - val_loss: 27.5515\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 26.50703\n",
            "Epoch 75/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 32.6491 - val_loss: 31.7316\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 26.50703\n",
            "Epoch 76/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 29.4689 - val_loss: 25.8829\n",
            "\n",
            "Epoch 00076: val_loss improved from 26.50703 to 25.88292, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 77/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 29.1688 - val_loss: 28.1405\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 25.88292\n",
            "Epoch 78/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 26.8080 - val_loss: 31.8198\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 25.88292\n",
            "Epoch 79/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 59.7146 - val_loss: 28.6270\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 25.88292\n",
            "Epoch 80/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 36.2988 - val_loss: 34.5187\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 25.88292\n",
            "Epoch 81/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 34.2817 - val_loss: 39.7382\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 25.88292\n",
            "Epoch 82/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 31.2504 - val_loss: 29.2598\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 25.88292\n",
            "Epoch 83/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 30.3943 - val_loss: 29.4893\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 25.88292\n",
            "Epoch 84/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 31.3142 - val_loss: 30.3239\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 25.88292\n",
            "Epoch 85/400\n",
            "205/205 [==============================] - 12s 61ms/step - loss: 29.2034 - val_loss: 29.9124\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 25.88292\n",
            "Epoch 86/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 28.0824 - val_loss: 36.0216\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 25.88292\n",
            "Epoch 87/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 37.2342 - val_loss: 28.4006\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 25.88292\n",
            "Epoch 88/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 30.3245 - val_loss: 29.6311\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 25.88292\n",
            "Epoch 89/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 30.6299 - val_loss: 27.5455\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 25.88292\n",
            "Epoch 90/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 29.0317 - val_loss: 31.1157\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 25.88292\n",
            "Epoch 91/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 29.1802 - val_loss: 26.6403\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 25.88292\n",
            "Epoch 92/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 29.4862 - val_loss: 27.9288\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 25.88292\n",
            "Epoch 93/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 28.6142 - val_loss: 33.8100\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 25.88292\n",
            "Epoch 94/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 29.3779 - val_loss: 29.7828\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 25.88292\n",
            "Epoch 95/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 36.4975 - val_loss: 28.1867\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 25.88292\n",
            "Epoch 96/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 34.0159 - val_loss: 29.3030\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 25.88292\n",
            "Epoch 97/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 31.8019 - val_loss: 30.8718\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 25.88292\n",
            "Epoch 98/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 29.8606 - val_loss: 30.6541\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 25.88292\n",
            "Epoch 99/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 28.7867 - val_loss: 28.4281\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 25.88292\n",
            "Epoch 100/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 30.7161 - val_loss: 56.5680\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 25.88292\n",
            "Epoch 101/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 28.0066 - val_loss: 29.0347\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 25.88292\n",
            "Epoch 102/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 35.8017 - val_loss: 42.5015\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 25.88292\n",
            "Epoch 103/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 34.7282 - val_loss: 33.2502\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 25.88292\n",
            "Epoch 104/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 32.7614 - val_loss: 33.7693\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 25.88292\n",
            "Epoch 105/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 31.1617 - val_loss: 30.0917\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 25.88292\n",
            "Epoch 106/400\n",
            "205/205 [==============================] - 13s 61ms/step - loss: 29.6061 - val_loss: 28.4240\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 25.88292\n",
            "Epoch 107/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 28.3713 - val_loss: 24.4081\n",
            "\n",
            "Epoch 00107: val_loss improved from 25.88292 to 24.40805, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 108/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 29.2402 - val_loss: 27.0925\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 24.40805\n",
            "Epoch 109/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 28.1151 - val_loss: 27.2904\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 24.40805\n",
            "Epoch 110/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 29.7411 - val_loss: 22.7085\n",
            "\n",
            "Epoch 00110: val_loss improved from 24.40805 to 22.70851, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 111/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 30.4959 - val_loss: 33.5851\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 22.70851\n",
            "Epoch 112/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 37.5399 - val_loss: 26.2941\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 22.70851\n",
            "Epoch 113/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 41.1462 - val_loss: 29.8194\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 22.70851\n",
            "Epoch 114/400\n",
            "205/205 [==============================] - 13s 64ms/step - loss: 31.0617 - val_loss: 26.5110\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 22.70851\n",
            "Epoch 115/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 28.1987 - val_loss: 26.1423\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 22.70851\n",
            "Epoch 116/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 27.7688 - val_loss: 27.9226\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 22.70851\n",
            "Epoch 117/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 26.1680 - val_loss: 27.7463\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 22.70851\n",
            "Epoch 118/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 24.5633 - val_loss: 25.2282\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 22.70851\n",
            "Epoch 119/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 29.0580 - val_loss: 26.7308\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 22.70851\n",
            "Epoch 120/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 25.6636 - val_loss: 26.1204\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 22.70851\n",
            "Epoch 121/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 23.7684 - val_loss: 26.5874\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 22.70851\n",
            "Epoch 122/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 29.0384 - val_loss: 27.8539\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 22.70851\n",
            "Epoch 123/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 23.8495 - val_loss: 21.3447\n",
            "\n",
            "Epoch 00123: val_loss improved from 22.70851 to 21.34474, saving model to /content/drive/My Drive/lstm_result/Model/[LSTM]jeonnam_VOLUME_WK_True_WS_20_FS_1_TS_50_U_256+256+128+128_A_softsign+tanh+tanh+softsign_E_400_B_1_D_0+0+0+0_2021-10-04_04-19.h5\n",
            "Epoch 124/400\n",
            "205/205 [==============================] - 13s 64ms/step - loss: 29.5157 - val_loss: 27.7229\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 21.34474\n",
            "Epoch 125/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 28.2859 - val_loss: 25.4943\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 21.34474\n",
            "Epoch 126/400\n",
            "205/205 [==============================] - 13s 63ms/step - loss: 24.9211 - val_loss: 24.6843\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 21.34474\n",
            "Epoch 127/400\n",
            "205/205 [==============================] - 13s 62ms/step - loss: 25.1371 - val_loss: 22.0431\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 21.34474\n",
            "Epoch 128/400\n",
            "  4/205 [..............................] - ETA: 12s - loss: 20.8911"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E6oTJcLYHbC"
      },
      "source": [
        "hist_dict = {\n",
        "    \"gangwon\": history_gangwon, \n",
        "    \"gyengnam\": history_gyengnam, \n",
        "    \"gyeonggi\": history_gyeonggi, \n",
        "    \"jeonnam\": history_jeonnam,\n",
        "}\n",
        "\n",
        "mms_y_dict = {\n",
        "    \"gangwon\": gangwon_mmsclaer_y, \n",
        "    \"gyengnam\": gyengnam_mmsclaer_y, \n",
        "    \"gyeonggi\": gyeonggi_mmsclaer_y, \n",
        "    \"jeonnam\": jeonnam_mmsclaer_y,\n",
        "}\n",
        "\n",
        "model_dict = {\n",
        "    \"gangwon\": model_gangwon, \n",
        "    \"gyengnam\": model_gyengnam, \n",
        "    \"gyeonggi\": model_gyeonggi, \n",
        "    \"jeonnam\": model_jeonnam,\n",
        "}\n",
        "\n",
        "path_dict = {\n",
        "    \"gangwon\": path_gangwon, \n",
        "    \"gyengnam\": path_gyengnam, \n",
        "    \"gyeonggi\": path_gyeonggi, \n",
        "    \"jeonnam\": path_jeonnam,\n",
        "}\n",
        "\n",
        "res_dict = {\n",
        "    \"gangwon\": res_gangwon, \n",
        "    \"gyengnam\": res_gyengnam, \n",
        "    \"gyeonggi\": res_gyeonggi, \n",
        "    \"jeonnam\": res_jeonnam,\n",
        "}\n",
        "\n",
        "scaled_dict = {\n",
        "    \"gangwon\": gangwon_scaled, \n",
        "    \"gyengnam\": gyengnam_scaled, \n",
        "    \"gyeonggi\": gyeonggi_scaled, \n",
        "    \"jeonnam\": jeonnam_scaled,\n",
        "}\n",
        "\n",
        "train_dict = {\n",
        "    \"gangwon\": train_gangwon, \n",
        "    \"gyengnam\": train_gyengnam, \n",
        "    \"gyeonggi\": train_gyeonggi, \n",
        "    \"jeonnam\": train_jeonnam,\n",
        "}\n",
        "\n",
        "x_dict = {\n",
        "    \"gangwon\": test_x_gangwon, \n",
        "    \"gyengnam\": test_x_gyengnam, \n",
        "    \"gyeonggi\": test_x_gyeonggi, \n",
        "    \"jeonnam\": test_x_jeonnam,\n",
        "}\n",
        "\n",
        "y_dict = {\n",
        "    \"gangwon\": test_y_gangwon, \n",
        "    \"gyengnam\": test_y_gyengnam, \n",
        "    \"gyeonggi\": test_y_gyeonggi, \n",
        "    \"jeonnam\": test_y_jeonnam,\n",
        "}\n",
        "\n",
        "for loc in LOCATION:\n",
        "    # 모델 결과\n",
        "    res_print(\n",
        "        loc=loc,\n",
        "        path=path_dict[loc],\n",
        "        model=model_dict[loc],\n",
        "        x=x_dict[loc],\n",
        "        y=y_dict[loc],\n",
        "        mms_y=mms_y_dict[loc],\n",
        "        evaluate=res_dict[loc],\n",
        "    )\n",
        "\n",
        "    # 모델 loss\n",
        "    graph_loss(\n",
        "        loc=loc,\n",
        "        history=hist_dict[loc],\n",
        "    )\n",
        "\n",
        "    # 예측 그래프\n",
        "    graph_predict(\n",
        "        loc=loc,\n",
        "        model_path=f\"{FOLDER_PATH}Model/{path_dict[loc]}.h5\",\n",
        "        model=model_dict[loc],\n",
        "        x=x_dict[loc],\n",
        "        y=y_dict[loc],\n",
        "        mms_y=mms_y_dict[loc],\n",
        "    )\n",
        "\n",
        "    # 학습 그래프\n",
        "    # graph_train(\n",
        "    #     loc=loc,\n",
        "    #     train_ds=train_dict[loc], \n",
        "    #     model_path=f\"{FOLDER_PATH}[LSTM]{path_dict[loc]}.h5\",\n",
        "    #     model=model_dict[loc],\n",
        "    #     mms_y=mms_y_dict[loc],\n",
        "    # )\n",
        "\n",
        "    # 학습 + 예측 그래프\n",
        "    graph_finished(\n",
        "        loc=loc,\n",
        "        scaled_ds=scaled_dict[loc], \n",
        "        train_ds=train_dict[loc],\n",
        "        model_path=f\"{FOLDER_PATH}Model/{path_dict[loc]}.h5\",\n",
        "        model=model_dict[loc],\n",
        "        mms_y=mms_y_dict[loc],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DPz4vpwPVxf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}